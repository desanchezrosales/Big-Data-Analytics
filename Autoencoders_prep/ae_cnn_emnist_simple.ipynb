{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders\n",
    "Usually when dealing with images, the encoder and decoder parts of an autoencoder are typically made of convolutional neural network layers.   This workbook will look at this more common autoencoder.\n",
    "\n",
    "One change when we read the data in: we need to shape it appropriately as a 28x28x1 image.\n",
    "\n",
    "Another change: since the CNN version takes **much** longer to train, we will do the following:\n",
    "1.  We train a short version in this workbook.   After training, we will save all of the relevant info (the models and the history information returned from training) to files.\n",
    "2.  We have a longer version that is almost exactly the same code, but running over all of the data.  The longer version is in python, and we have a pbs shell script you can submit to run on the batch system.  As above, after training, we will save all of the relevant info (the models and the history information returned from training) to files (named differently of course).\n",
    "3.  We will split the analysis part (looking at plots and so on) off into another workbook.  In this other workbook, you can use the files from this workbook, or the files that are made from the pbs/python version.  \n",
    "\n",
    "**Before** starting this workbook, open up a Pitzer shell, navigate to this directory, and type at the prompt:\n",
    "    \n",
    "    qsub pbs_run_ae_cnn.sh\n",
    "    \n",
    "This submits the longer python version of the code to the batch system.   Hopefully it will be close to finishing by the time you get to the end of this workbook.\n",
    "    \n",
    "After submitting the above script, you can go through this workbook.  Make sure you take some time to compare the python code run in the above script to the code in this workbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "#\n",
    "# See this for more info: https://arxiv.org/pdf/1702.05373.pdf\n",
    "mat = sio.loadmat('/fs/scratch/PAS1585/emnist/matlab/emnist-byclass.mat')\n",
    "#print(mat)\n",
    "\n",
    "data = mat['dataset']\n",
    "\n",
    "ex_train = data['train'][0,0]['images'][0,0]\n",
    "ey_train = data['train'][0,0]['labels'][0,0]\n",
    "ex_test = data['test'][0,0]['images'][0,0]\n",
    "ey_test = data['test'][0,0]['labels'][0,0]\n",
    "\n",
    "ex_train = ex_train.reshape( (ex_train.shape[0], 28,28), order='F')\n",
    "ex_test = ex_test.reshape( (ex_test.shape[0], 28,28), order='F')\n",
    "\n",
    "ex_train = ex_train.reshape( (ex_train.shape[0], 784))\n",
    "ex_test = ex_test.reshape( (ex_test.shape[0], 784))\n",
    "ex_train = ex_train.astype('float32') / 255.\n",
    "ex_test = ex_test.astype('float32') / 255.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(ex_train)\n",
    "df_train['label'] = ey_train\n",
    "df_digits_train = df_train[df_train['label']<=9]\n",
    "x_train = df_digits_train.iloc[:50000,:784].values\n",
    "x_train = x_train.reshape((x_train.shape[0],28,28,1))\n",
    "y_train = df_digits_train.iloc[:50000,784].values\n",
    "\n",
    "df_test = pd.DataFrame(ex_test)\n",
    "df_test['label'] = ey_test\n",
    "df_digits_test = df_test[df_test['label']<=9]\n",
    "x_test = df_digits_test.iloc[:,:784].values\n",
    "x_test = x_test.reshape((x_test.shape[0],28,28,1))\n",
    "y_test = df_digits_test['label'].values\n",
    "\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_labels_cat = to_categorical(y_train)\n",
    "y_test_labels_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit 0 ; count  5000\n",
      "digit 1 ; count  5538\n",
      "digit 2 ; count  4951\n",
      "digit 3 ; count  5034\n",
      "digit 4 ; count  5005\n",
      "digit 5 ; count  4559\n",
      "digit 6 ; count  4995\n",
      "digit 7 ; count  5146\n",
      "digit 8 ; count  4909\n",
      "digit 9 ; count  4863\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for digit,count in zip(unique, counts):\n",
    "    print(\"digit\",digit,\"; count \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the autoencoder.   \n",
    "Again we will use the keras [blog](https://blog.keras.io/building-autoencoders-in-keras.html) on autoencoders to guide us on the design.   However, we will use a syntax different from that source, and also explicitly design our auotoencoder model as made up of two **sub-models**.  Carefully examine the syntax below so you are sure it makes sense.\n",
    "\n",
    "Why do we do this?   After training, we can of course save the **autoencoder** model using the keras **save** method.  However, we can **also** save the sub-models (which we naturally call **encoder** and **decoder**).   This will be very important when we examine the use of **stacked** autoencoders in classification.\n",
    "\n",
    "In our CNN, we have:\n",
    "1.  An input layer: This is just the 784 pixels from the image.\n",
    "2.  The encoder: this layer has 784 inputs, and an output of dimension 4x4x8=128.  This is the same size as our encoder using the fully connected layers.  However, the encoder here uses 3 convolutional layers and 3 max-pooling layers.\n",
    "3.  The decoder: this layer takes the 128 outputs of the encoder as input, then has 784 outputs.  You will notice that the decoder uses **updampling**: UpSampling2D is just a simple scaling up of the image by resizing upwards.   This decoder uses 3 convolutional layers and 3 upsampling layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder===>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "=================================================================\n",
      "Total params: 1,904\n",
      "Trainable params: 1,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "decoder===>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 2,481\n",
      "Trainable params: 2,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "autoencoder===>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 4, 4, 8)           1904      \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 28, 28, 1)         2481      \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "# make our encoder\n",
    "encoder = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "encoder.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(28,28,1)))\n",
    "encoder.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "encoder.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "encoder.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "encoder.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "encoder.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "print(\"encoder===>\")\n",
    "print(encoder.summary())\n",
    "\n",
    "#\n",
    "# Now make the decoder\n",
    "# make our encoder\n",
    "decoder = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "decoder.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same',input_shape=(4,4,8)))\n",
    "decoder.add(layers.UpSampling2D((2,2)))\n",
    "decoder.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "decoder.add(layers.UpSampling2D((2,2)))\n",
    "decoder.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "decoder.add(layers.UpSampling2D((2,2)))\n",
    "decoder.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
    "print(\"decoder===>\")\n",
    "print(decoder.summary())\n",
    "\n",
    "#\n",
    "\n",
    "autoencoder = models.Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "#\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy',metrics=['mse'])\n",
    "#autoencoder.compile(optimizer='adadelta', loss='mse',metrics=['mse'])\n",
    "print(\"autoencoder===>\")\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 57918 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 28s 568us/step - loss: 0.3163 - mean_squared_error: 0.0711 - val_loss: 0.2477 - val_mean_squared_error: 0.0485\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 28s 555us/step - loss: 0.2334 - mean_squared_error: 0.0437 - val_loss: 0.2250 - val_mean_squared_error: 0.0411\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 28s 559us/step - loss: 0.2116 - mean_squared_error: 0.0360 - val_loss: 0.2007 - val_mean_squared_error: 0.0323\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 28s 567us/step - loss: 0.1985 - mean_squared_error: 0.0314 - val_loss: 0.1942 - val_mean_squared_error: 0.0301\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 28s 561us/step - loss: 0.1905 - mean_squared_error: 0.0286 - val_loss: 0.1867 - val_mean_squared_error: 0.0275\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.24773038675567796, 0.22498629634662895, 0.20069853438828514, 0.19421395036571076, 0.18672019371160128], 'val_mean_squared_error': [0.04853042836003452, 0.041131981605056524, 0.03227801555006427, 0.030074565491899154, 0.027463843469238525], 'loss': [0.316268920545578, 0.23344176127910615, 0.21158077536582948, 0.19851979088306426, 0.19051970526695253], 'mean_squared_error': [0.07107149774551391, 0.043657131626605984, 0.03600368348360062, 0.03144991469681263, 0.028598245741724967]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.24773038675567796, 0.22498629634662895, 0.20069853438828514, 0.19421395036571076, 0.18672019371160128], 'val_mean_squared_error': [0.04853042836003452, 0.041131981605056524, 0.03227801555006427, 0.030074565491899154, 0.027463843469238525], 'loss': [0.316268920545578, 0.23344176127910615, 0.21158077536582948, 0.19851979088306426, 0.19051970526695253], 'mean_squared_error': [0.07107149774551391, 0.043657131626605984, 0.03600368348360062, 0.03144991469681263, 0.028598245741724967]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('fully_trained_encoder_cnn_small.h5')\n",
    "decoder.save('fully_trained_decoder_cnn_small.h5')\n",
    "autoencoder.save('fully_trained_autoencoder_cnn_small.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our history data\n",
    "It is helpful to save the history data (note we only save the \"history.history\" part - otherwise the resulting saved object is **very** large) for plotting performance results later.   To do this we use the python **pickle** module.\n",
    "\n",
    "The pickle module implements an algorithm for serializing and de-serializing a Python object structure. “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream is converted back into an object hierarchy.  Note: **don't** try to use pickle for keras models!   Use the **save** method as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.24773038675567796, 0.22498629634662895, 0.20069853438828514, 0.19421395036571076, 0.18672019371160128], 'val_mean_squared_error': [0.04853042836003452, 0.041131981605056524, 0.03227801555006427, 0.030074565491899154, 0.027463843469238525], 'loss': [0.316268920545578, 0.23344176127910615, 0.21158077536582948, 0.19851979088306426, 0.19051970526695253], 'mean_squared_error': [0.07107149774551391, 0.043657131626605984, 0.03600368348360062, 0.03144991469681263, 0.028598245741724967]}\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "print(history.history)\n",
    "pickle.dump(history.history,open('history_cnn_small.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go the analysis workbook\n",
    "In this directory, open up \"ana_cnn.ipynb\" and continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
