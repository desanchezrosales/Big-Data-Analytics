{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analytics\n",
    "There are some problems which don't neatly fall into the category of traditional \"machine learning\" but can still be quite powerful.   The problem we will look at today involves relationships between things, and the tool we will use to investigate it is called **graph analytics** or **network analysis**.\n",
    "\n",
    "To illustrate this, we will start with a fun example: Marvel Comic Book Characters!\n",
    "![marvel](The_Marvel_Universe.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data\n",
    "Graphs are best illustrated by example, so for our data we will use a file containing a list of comic books and the \"heroes\" that appear in them.   This dataset is from https://www.kaggle.com/csanhueza/the-marvel-universe-social-network.\n",
    "\n",
    "The data is lines containing \"hero\" and \"comic book\".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "heroes_comics = pd.read_csv('/fs/scratch/PAS1585/graphs/heroes_comics.csv')\n",
    "print(heroes_comics.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming Links\n",
    "To link heroes, we will loop over the above dataframe, and do the following:\n",
    "* Count how often each hero appears\n",
    "* Keep track of which heroes appear in each individual comic book\n",
    "\n",
    "Once we have the data for each comic book, we can loop over the comic books and link heroes by the common comic books they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "def nested_defaultdict(default_factory, depth=1):\n",
    "    result = partial(defaultdict, default_factory)\n",
    "    for _ in repeat(None, depth - 1):\n",
    "        result = partial(defaultdict, result)\n",
    "    return result()\n",
    "\n",
    "from itertools import combinations \n",
    "  \n",
    "#\n",
    "# Loop over dataframe, and for each comic, store all of the heroes that appear in that comic\n",
    "comicHeroList = defaultdict(list)\n",
    "heroCount = defaultdict(int)\n",
    "for index, row in heroes_comics.iterrows():\n",
    "    hero = row['hero']\n",
    "    comic = row['comic']\n",
    "    heroCount[hero] += 1\n",
    "    comicHeroList[comic].append(hero)\n",
    "#\n",
    "# Now loop over comics, and count how often heroes show up together\n",
    "heroPairCount = nested_defaultdict(int,2)\n",
    "for comic in comicHeroList:\n",
    "    combos = combinations(comicHeroList[comic],2)\n",
    "    for (h1,h2) in combos:\n",
    "        heroPairCount[h1][h2] += 1\n",
    "        heroPairCount[h2][h1] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some printout\n",
    "Now lets print out the most common hero by count, and then for each of these, print out who they appear together with the most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hero in sorted(heroCount, key=heroCount.get, reverse=True)[:10]:\n",
    "    print(\"Hero: \",hero,\"; comic count \",heroCount[hero])\n",
    "    for hero2 in sorted(heroPairCount[hero], key=heroPairCount[hero].get, reverse=True)[:10]:\n",
    "        print(\"   appears with \",hero2,\"; number comics \",heroPairCount[hero][hero2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another way to view the data\n",
    "The tables above are interesting, but it is a little difficult to tell if all of the heroes are just randomnly connected, or if there is some structure to these connections.\n",
    "\n",
    "There is a way to reveal such structure if it exists.  The idea is to form a network.   A network has (at least) two primary features:\n",
    "* Nodes:  These are the primary \"objects\" in our network, and they are somewhat dependent on the problem you are dealing with.   In this case, each \"hero\" will be a node.  The size or weight of the node will be its total count.\n",
    "* Edges:  These are the connections between the nodes.   In our case, the edges are the defined by common comic books the heroes appear in.   If two heros appear together in at least one commic book, there will be a single edge between them.  The weight of that edge will be the count of these common appearances.\n",
    "\n",
    "The python package we will use is **networkx**.  In additon, we will use a community detection algorithm built for networkx called **louvain community detection**.   Before doing anything next, make sure you do the following:\n",
    "* pip install --user networkx\n",
    "* pip install --user python-louvain\n",
    "\n",
    "Then resstart the kernel and rerun the above code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "# Now look for communities\n",
    "G = nx.Graph()\n",
    "\n",
    "nodeCountCut = 50.0\n",
    "edgeCut = 25.0\n",
    "\n",
    "#\n",
    "# Now find edges that connect good nodes\n",
    "numEdges = 0\n",
    "numGoodEdges = 0\n",
    "goodEdgeNodes = set()\n",
    "nodeEdgeCount = defaultdict(int)\n",
    "for index1 in heroPairCount:\n",
    "    for index2 in heroPairCount[index1]:\n",
    "        if index1 != index2:\n",
    "            numEdges += 1\n",
    "            if heroPairCount[index1][index2] > edgeCut:\n",
    "                numGoodEdges += 1\n",
    "                G.add_edge(index1, index2, weight=heroPairCount[index1][index2])\n",
    "                goodEdgeNodes.add(index1)\n",
    "                goodEdgeNodes.add(index2)\n",
    "                nodeEdgeCount[index1] += 1\n",
    "                nodeEdgeCount[index2] += 1\n",
    "\n",
    "#\n",
    "# Next add to graph only those nodes that actually have at least one connection!\n",
    "numNodes = 0\n",
    "numGoodNodes = 0\n",
    "for hero in heroCount:\n",
    "    if hero in goodEdgeNodes:\n",
    "        G.add_node(hero,weight=heroCount[hero],classname=hero)\n",
    "        numNodes += 1\n",
    "        if nodeEdgeCount[hero]>0:\n",
    "            numGoodNodes += 1\n",
    "\n",
    "print(\"Total number all nodes  \",numNodes)\n",
    "print(\"Total number passing cuts nodes \",numGoodNodes)\n",
    "print(\"Total number all edges \",numEdges)\n",
    "print(\"Total number good edges \",numGoodEdges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection\n",
    "Once a network is made, a simple way to look for structure is to see if the nodes cluster.   One of the best tools for this is \"louvain community detection\" (https://perso.uclouvain.be/vincent.blondel/research/louvain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "# The smaller \"resolution\" is the more communities you get\n",
    "resolution = 1.5\n",
    "partition = community.best_partition(G,weight='weight', resolution=resolution)\n",
    "print(\"Number of found communities\",len(set(partition.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form Lookup Tables\n",
    "The following shows how to connect the found communities to the original list of heroes.\n",
    "\n",
    "The resulting tables of the largest hero communities and their members are printed, and the results look really sensible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communityList = defaultdict(list)\n",
    "communityCount = defaultdict(int)\n",
    "communityHeroCount = nested_defaultdict(int,2)\n",
    "for communityID,hero in zip(partition.values(),partition.keys()):\n",
    "#    print(\"communityID \",communityID,\"; communityIndex \",hero)\n",
    "    communityList[communityID].append(hero)\n",
    "    communityCount[communityID] += 1\n",
    "    communityHeroCount[communityID][hero] = heroCount[hero]\n",
    "\n",
    "for communityID in sorted(communityCount, key=communityCount.get, reverse=True)[:10]:\n",
    "    print(\"Community ID \",communityID,\"; number of members \",communityCount[communityID])\n",
    "    for hero in sorted(communityHeroCount[communityID], key=communityHeroCount[communityID].get, reverse=True)[:10]:\n",
    "        print(\"   hero \",hero,\"count \",communityHeroCount[communityID][hero])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Network\n",
    "The structure revealed above is probably the simplest (and also probably the most valuable) way to reveal information about a network.   One thing that is nice to explore is a visualization of our network, and of the communities that we have found.   Unfortunately, the only thing we have time to explore are tools involving matplotlib, which are fairly limited.  \n",
    "\n",
    "First some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=0.75)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos\n",
    "\n",
    "def test():\n",
    "    # to install networkx 2.0 compatible version of python-louvain use:\n",
    "    # pip install -U git+https://github.com/taynaud/python-louvain.git@networkx2\n",
    "    from community import community_louvain\n",
    "\n",
    "    g = nx.karate_club_graph()\n",
    "    partition = community_louvain.best_partition(g, resolution=1.5)\n",
    "    print(\"Number of found communities\",len(set(partition.values())))\n",
    "    pos = community_layout(g, partition)\n",
    "    values = [partition.get(node) for node in g.nodes()]\n",
    "    nx.draw(g, pos, node_color=values)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"Karate Club\" example.  \n",
    "Zachary's Karate Club is a small example network included with networkx which can be used to test a number of features of graph analytics, including community detection.   See this (https://en.wikipedia.org/wiki/Zachary%27s_karate_club) for more details.\n",
    "\n",
    "Running the \"test()\" method of the above code, shows this simple graph after community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the Marvel Network\n",
    "Now lets use the same tool to draw our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)\n",
    "pos = community_layout(G, partition)\n",
    "values = [partition.get(node) for node in G.nodes()]\n",
    "nx.draw(G, pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False, font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of graph analytics to real-world numerical datasets\n",
    "The above analysis is interesting, but there are not many problems which we will encounter in a typical analysis environment which are like the above Marvel Universe.   It is reasonable to ask if these tools might be applicable in other circumstances.\n",
    "\n",
    "The idea of community detection seems like one that might have such broader application.   Community detction is just a form of clustering, which itself is an example of \"unsupervised\" learning.   Can we use community detection in a real work example?  The answer is yes.\n",
    "\n",
    "To do this we will use \"fake\" datasets (generated using sklearn tools), as well as some helper functions in the code block below.  The helper fucntions will allow us to visualize the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import more_itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.options.display.max_rows=900\n",
    "pd.options.display.max_columns=900\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "def visualize_3d(X,y,labels,algorithm=\"tsne\",title=\"Data in 3D\"):\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    if algorithm==\"tsne\":\n",
    "        reducer = TSNE(n_components=3,random_state=47,n_iter=400,angle=0.6)\n",
    "    elif algorithm==\"pca\":\n",
    "        reducer = PCA(n_components=3,random_state=47)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensionality reduction algorithm given.\")\n",
    "    \n",
    "    if X.shape[1]>3:\n",
    "        X = reducer.fit_transform(X)\n",
    "    else:\n",
    "        if type(X)==pd.DataFrame:\n",
    "        \tX=X.values\n",
    "    \n",
    "    marker_shapes = [\"circle\",\"diamond\", \"circle-open\", \"square\",  \"diamond-open\", \"cross\",\"square-open\",]\n",
    "    traces = []\n",
    "    for hue in np.unique(y):\n",
    "        X1 = X[y==hue]\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "            x=X1[:,0],\n",
    "            y=X1[:,1],\n",
    "            z=X1[:,2],\n",
    "            mode='markers',\n",
    "            name = str(hue),\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                symbol=marker_shapes.pop(),\n",
    "                line=dict(\n",
    "                    width=int(np.random.randint(3,10)/10)\n",
    "                ),\n",
    "                opacity=int(np.random.randint(6,10)/10)\n",
    "            )\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title='Dim 1'),\n",
    "            yaxis=dict(\n",
    "                title='Dim 2'),\n",
    "            zaxis=dict(\n",
    "                title='Dim 3'), ),\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "    \n",
    "def visualize_3dnew(X,y,labels,algorithm=\"tsne\",title=\"Data in 3D\"):\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    if algorithm==\"tsne\":\n",
    "        reducer = TSNE(n_components=3,random_state=47,n_iter=400,angle=0.6)\n",
    "    elif algorithm==\"pca\":\n",
    "        reducer = PCA(n_components=3,random_state=47)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensionality reduction algorithm given.\")\n",
    "    \n",
    "    if X.shape[1]>3:\n",
    "        X = reducer.fit_transform(X)\n",
    "    else:\n",
    "        if type(X)==pd.DataFrame:\n",
    "        \tX=X.values\n",
    "    \n",
    "    marker_shapes = [\"circle\",\"diamond\", \"circle-open\", \"square\",  \"diamond-open\", \"cross\",\"square-open\",]\n",
    "    DEFAULT_PLOTLY_COLORS=['rgb(31, 119, 180)', 'rgb(255, 127, 14)',\n",
    "                       'rgb(44, 160, 44)', 'rgb(214, 39, 40)',\n",
    "                       'rgb(148, 103, 189)', 'rgb(140, 86, 75)',\n",
    "                       'rgb(227, 119, 194)', 'rgb(127, 127, 127)',\n",
    "                       'rgb(188, 189, 34)', 'rgb(23, 190, 207)']\n",
    "    traces = []\n",
    "    for hue in np.unique(y):\n",
    "        X1 = X[y==hue]\n",
    "        thisText = y[y==hue]\n",
    "        theseColors = []\n",
    "        for lbl in labels[y==hue]:\n",
    "            if lbl>len(DEFAULT_PLOTLY_COLORS):\n",
    "                color = DEFAULT_PLOTLY_COLORS[0]\n",
    "            else:\n",
    "                color = DEFAULT_PLOTLY_COLORS[lbl]\n",
    "            theseColors.append(color)\n",
    "            #theseColors.append('rgb(188, 189, 34)')\n",
    "        trace = go.Scatter3d(\n",
    "            x=X1[:,0],\n",
    "            y=X1[:,1],\n",
    "            z=X1[:,2],\n",
    "            mode='markers+text',\n",
    "            text=thisText,\n",
    "            name = str(hue),\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                symbol=marker_shapes.pop(),\n",
    "                line=dict(\n",
    "                    width=int(np.random.randint(3,10)/10)\n",
    "                ),\n",
    "                color=theseColors,\n",
    "                opacity=int(np.random.randint(6,10)/10)\n",
    "            )\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title='Dim 1'),\n",
    "            yaxis=dict(\n",
    "                title='Dim 2'),\n",
    "            zaxis=dict(\n",
    "                title='Dim 3'), ),\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "    \n",
    "def visualize_2d(X,y,algorithm=\"tsne\",title=\"Data in 2D\",figsize=(8,8)):\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    if algorithm==\"tsne\":\n",
    "        reducer = TSNE(n_components=2,random_state=47,n_iter=400,angle=0.6)\n",
    "    elif algorithm==\"pca\":\n",
    "        reducer = PCA(n_components=2,random_state=47)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensionality reduction algorithm given.\")\n",
    "    if X.shape[1]>2:\n",
    "        X = reducer.fit_transform(X)\n",
    "    else:\n",
    "        if type(X)==pd.DataFrame:\n",
    "        \tX=X.values\n",
    "    f, (ax1) = plt.subplots(nrows=1, ncols=1,figsize=figsize)\n",
    "    sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax1);\n",
    "    ax1.set_title(title);\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake Data\n",
    "sklearn comes with a powerful tool for generating datasets call \"make_classification\".   The resulting generated datasets can then be used to test other algorithms such as classification and regression.   We will use this to test the clustering ability of graph analytics algorithms.\n",
    "\n",
    "The important parameters for \"make_classification\" are the following:\n",
    "* n_samples: this is the total amount of data you want generated\n",
    "* n_classes: the total number of different classes you want your data to have\n",
    "* n_features: the total number of features that describe each data point in your sample\n",
    "* class_sep: the larger this value, the more your classes are separated (and presumably the easier they are to classify or cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "X,Y = make_classification(n_samples=1000, n_features=10,\n",
    "                    n_redundant=1, n_repeated=0, n_classes=4, n_clusters_per_class=1,\n",
    "                    class_sep=1.2,\n",
    "                   flip_y=0)\n",
    "Xd = pd.DataFrame(X)\n",
    "Yd = pd.Series(Y)\n",
    "visualize_3dnew(Xd,Yd,Yd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of data points per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "def nested_defaultdict(default_factory, depth=1):\n",
    "    result = partial(defaultdict, default_factory)\n",
    "    for _ in repeat(None, depth - 1):\n",
    "        result = partial(defaultdict, result)\n",
    "    return result()\n",
    "\n",
    "\n",
    "classNums = defaultdict(int)\n",
    "for y in Y:\n",
    "    classNums[y] += 1\n",
    "print(\"Class numbers \",classNums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges\n",
    "We need some way to define edges, or connect, our datapoints.   To do this, we will use a concept we introduced previously: **cosine similarity**.   Our points each lie in an n-dimensional space (defined by the features).   By calculating the cosine between each point, we can define a strength of the connection:\n",
    "* if the cosine>threshold then the points are connected\n",
    "* if the cosing<threshold, then the points are not connected\n",
    "\n",
    "We are assuming that since the data is generated to come from different classes, that the cosine of points ffrom the same classes should be closer than points from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "simSame = []\n",
    "simDifferent = []\n",
    "\n",
    "\n",
    "labels = []\n",
    "index = 0\n",
    "for y in Y:\n",
    "    labels.append(index)\n",
    "    index += 1\n",
    "\n",
    "edges = nested_defaultdict(float,2)\n",
    "for x1,y1,index1 in zip(X,Y,labels):\n",
    "    for x2,y2,index2 in zip(X,Y,labels):\n",
    "        if index2>index1:\n",
    "            sim = cosine_similarity(x1,x2)\n",
    "            edges[index1][index2] = sim\n",
    "            edges[index2][index1] = sim\n",
    "            if y1==y2:\n",
    "                simSame.append(sim)\n",
    "            else:\n",
    "                simDifferent.append(sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare cosing similarity for points\n",
    "Look at points of the same class, versus points of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "\n",
    "pyplot.hist(simSame, bins, alpha=0.5, label='same')\n",
    "pyplot.hist(simDifferent, bins, alpha=0.5, label='different')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the connections\n",
    "From the curve above, it looks like we can call connected points, those that have cosine>0.5.\n",
    "\n",
    "With this definition, we can form a graph.\n",
    "* Edges are nodes with have cosine>threshold (which we set to be 0.5)\n",
    "* Nodes are those which have at least some threshold of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "# Now look for communities\n",
    "G = nx.Graph()\n",
    "\n",
    "nodeCountCut = 5.0\n",
    "edgeCut = 0.5\n",
    "\n",
    "#\n",
    "# Now find edges that connect good nodes\n",
    "numEdges = 0\n",
    "numGoodEdges = 0\n",
    "goodEdgeNodes = set()\n",
    "nodeEdgeCount = defaultdict(int)\n",
    "for index1 in edges:\n",
    "    for index2 in edges[index1]:\n",
    "        if index1 != index2:\n",
    "            numEdges += 1\n",
    "            if edges[index1][index2] > edgeCut:\n",
    "                numGoodEdges += 1\n",
    "                G.add_edge(index1, index2, weight=edges[index1][index2])\n",
    "                goodEdgeNodes.add(index1)\n",
    "                goodEdgeNodes.add(index2)\n",
    "                nodeEdgeCount[index1] += 1\n",
    "                nodeEdgeCount[index2] += 1\n",
    "\n",
    "#\n",
    "# Next add to graph only those nodes that actually have at least one connection!\n",
    "numNodes = 0\n",
    "numGoodNodes = 0\n",
    "for x,y,index in zip(X,Y,labels):\n",
    "    G.add_node(index,weight=nodeEdgeCount[index],classname=y)\n",
    "    numNodes += 1\n",
    "    if nodeEdgeCount[index]>0:\n",
    "        numGoodNodes += 1\n",
    "\n",
    "print(\"Total number all nodes  \",numNodes)\n",
    "print(\"Total number passing cuts nodes \",numGoodNodes)\n",
    "print(\"Total number all edges \",numEdges)\n",
    "print(\"Total number good edges \",numGoodEdges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "# The smaller \"resolution\" is the more communities you get\n",
    "resolution = 1.0\n",
    "partition = community.best_partition(G,weight='weight', resolution=resolution)\n",
    "print(\"Number of found communities\",len(set(partition.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Communities to Classes\n",
    "The found communities are the same as the number of classes.   Remember: this was totally unsupervised!   Now we need to see if the communities actually correspond to the true classes.\n",
    "\n",
    "This is a little more complicated than the marvel universe.  In our case, each point already belowngs to a class, and we want to know how often our classes are connected to the same community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexToCommunity = {}\n",
    "for communityID,communityIndex in zip(partition.values(),partition.keys()):\n",
    "    indexToCommunity[communityIndex] = communityID\n",
    "    \n",
    "communityClassCount = nested_defaultdict(int,2)\n",
    "classCommunityCount = nested_defaultdict(int,2)\n",
    "communityAssignment = []\n",
    "for x,y,index in zip(X,Y,labels):\n",
    "    if index in indexToCommunity:\n",
    "#        print(\"Data label \",index,\"; class \",y)\n",
    "        community = indexToCommunity[index]\n",
    "#        print(\"Data label \",index,\"; class \",y,\"; assigned community \",community)\n",
    "        communityClassCount[community][y] += 1\n",
    "        classCommunityCount[y][community] += 1\n",
    "        communityAssignment.append(community)\n",
    "communityAssignment = np.asarray(communityAssignment)\n",
    "print()\n",
    "print(\"Top classes for each community\")\n",
    "for community in communityClassCount:\n",
    "    print(\"community\",community)\n",
    "    for y in communityClassCount[community]:\n",
    "        print(\"   class \",y,\" count \",communityClassCount[community][y])\n",
    "        \n",
    "print()\n",
    "print(\"Top communities for each class\")\n",
    "for y in classCommunityCount:\n",
    "    print(\"class\",y)\n",
    "    for community in classCommunityCount[y]:\n",
    "        print(\"   community \",community,\" count \",classCommunityCount[y][community])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3dnew(Xd,Yd,communityAssignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)\n",
    "pos = community_layout(G, partition)\n",
    "values = [partition.get(node) for node in G.nodes()]\n",
    "nx.draw(G, pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False, font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [partition.get(node) for node in G.nodes()]\n",
    "\n",
    "nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (4 pts total)\n",
    "Now let's apply this to a data sample we used previously: pulsars.   We I want you to do is to \n",
    "* calculate the cosine similarity among all points in the pulsar dataset\n",
    "* plot the cosine similary for true pulsars (one class) vs non-pulsars (the other class).\n",
    "* Run community detection and see how well the communities line up with the true classes\n",
    "* Plot the communities using visualize_3dnew\n",
    "\n",
    "The code below sets things up by reading the data in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#\n",
    "# Read in all of the other digits\n",
    "fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/HTRU2/HTRU_2a.csv'\n",
    "dfAll = pd.read_csv(fname)\n",
    "print(dfAll.head(5))\n",
    "#\n",
    "# The data already has a 0/1 class variable that defines signal (1) and background (0)\n",
    "#\n",
    "# The data is already combined but it will be usefull to split it so we can look at \n",
    "# signal and background separately.\n",
    "dfA = dfAll[dfAll['class']==1]\n",
    "dfB = dfAll[dfAll['class']==0]\n",
    "\n",
    "print(\"Length of signal sample:     \",len(dfA))\n",
    "print(\"Length of background sample: \",len(dfB))\n",
    "\n",
    "#\n",
    "# Shuffle the data here\n",
    "from sklearn.utils import shuffle\n",
    "dfBShuffle = shuffle(dfB)\n",
    "#\n",
    "# Uncomment the next line to limit dfB to be the same length as dfA\n",
    "#dfB_use = dfBShuffle\n",
    "dfB_use = dfBShuffle.head(len(dfA))\n",
    "\n",
    "\n",
    "dfCombined = dfB_use\n",
    "dfCombined = pd.concat([dfCombined, dfA])\n",
    "dfCombined = shuffle(dfCombined)\n",
    "\n",
    "print(\"Size of signal sample \",len(dfA))\n",
    "print(\"Size of background sample \",len(dfB_use))\n",
    "print(\"Size of combined sample \",len(dfCombined))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
    "\n",
    "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:8])\n",
    "Y = dfCombinedShuffle['class'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
