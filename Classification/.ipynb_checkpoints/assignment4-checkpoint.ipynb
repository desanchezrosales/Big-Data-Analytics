{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-XII8KKiNzp"
   },
   "source": [
    "# Assignment 4\n",
    "In the previous workbook, we had a classifier which was designed to picks between two specific digits, one that we called **signal** and the other we called **background**.   \n",
    "\n",
    "In this assignment, we will want to read in all of the digits, and design a classifier which finds a specific digit (our **signal** again), but **all** of the other 9 digits will serve as the background.   Our background will naturally be 9 times bigger than our signal (unless we limit it).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvhYHv5X2K_p"
   },
   "source": [
    "# Some useful methods\n",
    "These two methods will be useful for the code blocks we will write.\n",
    "\n",
    "The \"nested_defaultdict\" function is very useful for making two dimensional counter (actually nested dictionaries).   A sample use follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qH7R-6S2OVm"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "def nested_defaultdict(default_factory, depth=1):\n",
    "    result = partial(defaultdict, default_factory)\n",
    "    for _ in repeat(None, depth - 1):\n",
    "        result = partial(defaultdict, result)\n",
    "    return result()  \n",
    "\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVViuD5c55rQ"
   },
   "source": [
    "# Test of nested_defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfwZY1qQyB0a"
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "#\n",
    "# Define our multi-dimensional counter\n",
    "digitFoundCounter = nested_defaultdict(int,2)\n",
    "#\n",
    "# Here is a for loop that runs from 0 to 9 (1 less than the argument of \"range\")\n",
    "for i in range(1000):\n",
    "#\n",
    "# Generate a random digit\n",
    "  digit = rd.randint(0, 9)\n",
    "  digitFoundCounter[digit]['generated'] += 1\n",
    "#\n",
    "# For that digit, call it \"found\" if another random number is <0.8 (so ~80% efficient)\n",
    "  if rd.random() < 0.8:\n",
    "    digitFoundCounter[digit]['found'] += 1\n",
    "    \n",
    "#\n",
    "# Now print results\n",
    "for digit in range(10):\n",
    "  print(\"Digit \",digit,\"; generated \",digitFoundCounter[digit]['generated'],\"; found \",digitFoundCounter[digit]['found'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvfjJRFB0yRQ"
   },
   "source": [
    "# Getting the data:\n",
    "As usual the data is on github.   If you uncomment the line with **short = \"short_\"**, the code will run faster.  Once it is working you should use the full sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luIJ0Dguwewg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#\n",
    "short = \"\"\n",
    "#short = \"short_\"\n",
    "\n",
    "#\n",
    "# Read in all of the other digits\n",
    "dfAll = pd.DataFrame()\n",
    "for digit in range(10):\n",
    "    print(\"Processing digit \",digit)\n",
    "    fname = '/fs/scratch/PAS1585/ch3/digit_' + short + str(digit) + '.csv'\n",
    "    df = pd.read_csv(fname,header=None)\n",
    "    df['digit'] = digit\n",
    "    dfAll = pd.concat([dfAll, df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0m8yoxK0ef1"
   },
   "source": [
    "# Defining Signal\n",
    "At the top of this block we define which of the 10 digits we want to use for our **signal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrahohxi0d-A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZVQuKLBTV3T"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define our \"signal\" digit\n",
    "digitSignal = 5\n",
    "\n",
    "dfA = dfAll[dfAll['digit']==digitSignal]\n",
    "dfB = dfAll[dfAll['digit']!=digitSignal]\n",
    "\n",
    "\n",
    "dfA['signal'] = 1\n",
    "dfB['signal'] = 0\n",
    "\n",
    "print(\"Length of signal sample:     \",len(dfA))\n",
    "print(\"Length of background sample: \",len(dfB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7XCcD7Zxzzp"
   },
   "source": [
    "# Task1: Shuffle background\n",
    "You will need to **shuffle** or randomize the rows of the background data.  We already read the digits in, but they were in batches of the the same digit.   So we need to shuffle them to mix the digits up.  To igure out how to do this, google **sklearn shuffle pandas**.   Note that the \"shuffle\" method from sklearn creates copies of the orginal dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TydTfq902ypY"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpeG7BJ92sGE"
   },
   "source": [
    "# Tasks 2: Limit background\n",
    "Come up with a method to limit the rows of the background data that you use, so that it is the **same length** (in rows) as the signal dataframe.   You will want some easy way to turn this on and off.   Run first with the background limited to the **same** length as the signal.   Later you can come back and use all of the background data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5r19e7UKZ1I"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jY8MMt_YKeP3"
   },
   "source": [
    "# Task 3: Combining data\n",
    "After steps 1 and 2 , you will have a signal dataframe, and a randomized background dataframe.   You will need to combine these two into a single dataframe.  We have done this before, but you can look at **pandas concat** function.   Use the name **dfCombined** for the combined dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQToio4HysmK"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIhQBrAXz3V8"
   },
   "source": [
    "#  Task 4: runFitter Method\n",
    "Next you will want to apply an estimator to this dataset. I want you to make a function that does both the test/train split, and then calls the estimator. The function should look like the following \"skeleton\".   I show the expected inputs and the expected return values.   Note we did all of this in the example workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrB1nvOxz5gq"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here is the skeleton of the method\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# The inputs are:\n",
    "#     dfCombined: the input dataframe\n",
    "#     estimator:  this should be an sklearn classifier (only LinearSVC or SGDClassifier are expected to be used)\n",
    "def runFitter(dfCombined,estimator):\n",
    "#\n",
    "# First do a test/train split\n",
    "\n",
    "\n",
    "#\n",
    "# Now fit to our training set\n",
    "\n",
    "\n",
    "#\n",
    "# Now predict the classes and get the score for our traing set\n",
    "\n",
    "\n",
    "#\n",
    "# Now predict the classes and get the score for our test set\n",
    "\n",
    "\n",
    "#\n",
    "  return y_train,y_train_pred,y_train_score,y_train_truedigit,y_test,y_test_pred,y_test_score,y_test_truedigit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFBJwh-X0L0J"
   },
   "source": [
    "# Task 5: Run the fitter\n",
    "Now we can use the function we defined above.   We have to define our estimate (which we get from sklearn) outside of the method, and pass it as an argument to our defined function.   We do it this way because later on we will want to call the method with a different estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-zEJBYF0NW_"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "estimator = LinearSVC(random_state=42) # use dual=False when  n_samples > n_features which is what we have\n",
    "#estimator = LinearSVC(random_state=42,dual=False,max_iter=5000)    # use dual=False when  n_samples > n_features which is what we have\n",
    "\n",
    "y_train,y_train_pred,y_train_score,y_train_truedigit,y_test,y_test_pred,y_test_score,y_test_truedigit = runFitter(dfCombined,estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-j8Wd0b0akR"
   },
   "source": [
    "# Task 5: Implement Performance Method\n",
    "Next, you need to get the performance of the estimator on **both** the training and testing data.   To do this, I want you to make a function to calulate various performance metrics, and return the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ALFWAtg0csD"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Determine the performance\n",
    "#\n",
    "# The inputs:\n",
    "#    y = array of true labels\n",
    "#    y_pred = array of predicted labels from the **predict** method of the LinearSVC estimator\n",
    "#    y_score = array of scores from the **decision_function** method of the LinearSVC estimator\n",
    "#\n",
    "# The return values:\n",
    "#    precision,recall,auc are the calculated values of these metrics\n",
    "#    fpr, tpr, thresholds are lists containing the  \"false positive rate\", \"true positve rate\", and \"threshold\"\n",
    "#\n",
    "def binaryPerformance(y,y_pred,y_score):\n",
    "#\n",
    "# Assuming a binary classifier with 1=signal, 0=background\n",
    "  confusionMatrix = nested_defaultdict(int,2)\n",
    "#\n",
    "# Get entries in confusion matrix, then calculate precision and recall\n",
    "# The two dimensions of \"confusionMatrix\" will be confusionMatrix[trueClass][predClass]\n",
    "  \n",
    "#\n",
    "# Get the ROC curve.  We will use the sklearn function to do this\n",
    "  from sklearn import metrics\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y, y_score, pos_label=1)\n",
    "\n",
    "#\n",
    "# Get the auc:   \n",
    "  auc = metrics.roc_auc_score(y, y_score)\n",
    "  \n",
    "  return precision,recall,auc,fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4lHvlzF4RsT"
   },
   "source": [
    "# Task 6: Call performance method\n",
    "Call the \"binaryPerformance\" method for both the training and testing results for your estimator. How do they compare? Look at precision, recall, AUC, and the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Jt3lSPr7ZrV"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now get the performaance by calling your binaryPerformance method\n",
    "#   ===> do this separately for test and train data!\n",
    "# Here is how you do this for the test sample:\n",
    "precision_test,recall_test,auc_test,fpr_test, tpr_test, thresholds_test = binaryPerformance(y_test,y_test_pred,y_test_score,debug=False)\n",
    "\n",
    "#\n",
    "# Now print\n",
    "print(\"AUC training data: \",auc_train)\n",
    "print(\"AUC testing data:  \",auc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbSIhu_94xNw"
   },
   "source": [
    "# Plot the ROC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGId2jgZ8trd"
   },
   "outputs": [],
   "source": [
    "#import plotly.plotly as py\n",
    "import numpy as np\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x=fpr_train,\n",
    "    y=tpr_train,\n",
    "    text=thresholds_train,\n",
    "    mode='line',\n",
    "    name='Trainig set'\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=fpr_test,\n",
    "    y=tpr_test,\n",
    "    text=thresholds_test,\n",
    "    mode='line',\n",
    "    name='Testing set'\n",
    ")\n",
    "\n",
    "layout = dict(\n",
    "    title='ROC Curve',\n",
    "    xaxis=dict(title='FPR'),\n",
    "    yaxis=dict(title='TPR')\n",
    ")\n",
    "\n",
    "data = [trace0,trace1]      #   this is a list because you might want to plot many data sets\n",
    "iplot(dict(data=data))#,layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7UeI_rEMEV4"
   },
   "source": [
    "# Task 7: A different Classifier\n",
    "Run with a different classifier (and the large background statistics): the SGDClassifier. Compare your results to the LinearSVC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9k3g50UJMOdc"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# Get the new estimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "estimatorSGD = SGDClassifier(random_state=42) \n",
    "#estimatorSGD = SGDClassifier(random_state=42,class_weight=\"balanced\") \n",
    "#\n",
    "# Call runFitter\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Now get the performance by calling binaryPerformance\n",
    "\n",
    "\n",
    "print(\"AUC training data: \",auc_train)\n",
    "print(\"AUC testing data:  \",auc_test)\n",
    "print(\"AUC SGD training data: \",auc_sgd_train)\n",
    "print(\"AUC SGD testing data:  \",auc_sgd_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxjlDFiO5Qrn"
   },
   "source": [
    "# Plot all 4 ROCs!\n",
    "There are 2 from the original estimator, and 2 from the new estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ka-IYMyxNN1U"
   },
   "outputs": [],
   "source": [
    "# Plot all 4 results\n",
    "import plotly.plotly as py\n",
    "import numpy as np\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WV9ykc2Vqci1"
   },
   "source": [
    "# Additional tasks:\n",
    "1.   For a given signal digit, is the accuracy with which background is rejected, dependent upon what the background digit is?   For example, imagine that our signal digit is 4.   Is th accuracy that a 0 is identifed as background the same as the accuracy that a 9 is identified as background?\n",
    "2.   Limit the signal to 1/10 of its amximum, and the backgorund to the same number.   What is the perfromance of the estimator?   Use LinearSVC for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment4.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
