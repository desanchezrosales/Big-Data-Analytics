{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras\n",
    "In this notebook, we will begin our introduction to keras.   We will follow Chapter 2 of **Deep Learning with Python** for most of this, with some slight changes.\n",
    "\n",
    "We will build a simple model to start with, implementing a network just like the one we used in assignment7_prep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Sure Your Environment is setup properly\n",
    "First, make sure that your **kernel** is Python 3.6 (Conda 5.2).   This is in the upper right hand corner of this notebook.   If it is not - change it by selecting the **Kernel** menu option and navigating to **Change Kernel**.\n",
    "\n",
    "Next, execute the following cell, using **pip list** to determine if you have the proper Keras and tensorflow packages.   You should have:\n",
    "1. Keras                              2.2.4\n",
    "2. tensorflow-gpu                     1.9.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version   \r\n",
      "---------------------------------- ----------\r\n",
      "absl-py                            0.8.0     \r\n",
      "alabaster                          0.7.11    \r\n",
      "anaconda-clean                     1.0       \r\n",
      "anaconda-client                    1.7.1     \r\n",
      "anaconda-navigator                 1.8.7     \r\n",
      "anaconda-project                   0.8.2     \r\n",
      "appdirs                            1.4.3     \r\n",
      "argcomplete                        1.9.4     \r\n",
      "asn1crypto                         0.24.0    \r\n",
      "astor                              0.8.0     \r\n",
      "astroid                            2.0.4     \r\n",
      "astropy                            3.0.4     \r\n",
      "atomicwrites                       1.1.5     \r\n",
      "attrs                              18.1.0    \r\n",
      "Automat                            0.7.0     \r\n",
      "Babel                              2.6.0     \r\n",
      "backcall                           0.1.0     \r\n",
      "backports.shutil-get-terminal-size 1.0.0     \r\n",
      "backports.weakref                  1.0.post1 \r\n",
      "beautifulsoup4                     4.6.3     \r\n",
      "bitarray                           0.8.3     \r\n",
      "bkcharts                           0.2       \r\n",
      "blaze                              0.11.3    \r\n",
      "bleach                             1.5.0     \r\n",
      "bokeh                              0.13.0    \r\n",
      "boto                               2.49.0    \r\n",
      "Bottleneck                         1.2.1     \r\n",
      "certifi                            2018.10.15\r\n",
      "cffi                               1.11.5    \r\n",
      "chardet                            3.0.4     \r\n",
      "chest                              0.2.3     \r\n",
      "click                              6.7       \r\n",
      "cloudpickle                        0.5.3     \r\n",
      "clyent                             1.2.2     \r\n",
      "colorama                           0.3.9     \r\n",
      "conda                              4.5.11    \r\n",
      "conda-build                        3.13.0    \r\n",
      "conda-verify                       3.1.0     \r\n",
      "configobj                          5.0.6     \r\n",
      "constantly                         15.1.0    \r\n",
      "contextlib2                        0.5.5     \r\n",
      "cryptography                       2.3       \r\n",
      "cryptography-vectors               2.3       \r\n",
      "cycler                             0.10.0    \r\n",
      "Cython                             0.28.5    \r\n",
      "cytoolz                            0.9.0.1   \r\n",
      "dask                               0.18.2    \r\n",
      "datashape                          0.5.4     \r\n",
      "decorator                          4.3.0     \r\n",
      "dill                               0.2.8.2   \r\n",
      "distributed                        1.22.1    \r\n",
      "docutils                           0.14      \r\n",
      "entrypoints                        0.2.3     \r\n",
      "et-xmlfile                         1.0.1     \r\n",
      "fastcache                          1.0.2     \r\n",
      "filelock                           3.0.4     \r\n",
      "flake8                             3.5.0     \r\n",
      "Flask                              1.0.2     \r\n",
      "Flask-Cors                         3.0.6     \r\n",
      "funcsigs                           1.0.2     \r\n",
      "future                             0.16.0    \r\n",
      "gast                               0.2.2     \r\n",
      "gevent                             1.3.6     \r\n",
      "glob2                              0.6       \r\n",
      "gmpy2                              2.0.8     \r\n",
      "google-pasta                       0.1.7     \r\n",
      "graphviz                           0.13      \r\n",
      "greenlet                           0.4.14    \r\n",
      "grpcio                             1.24.0    \r\n",
      "h5py                               2.8.0     \r\n",
      "heapdict                           1.0.0     \r\n",
      "html5lib                           0.9999999 \r\n",
      "hyperlink                          18.0.0    \r\n",
      "idna                               2.7       \r\n",
      "imageio                            2.3.0     \r\n",
      "imagesize                          1.0.0     \r\n",
      "incremental                        17.5.0    \r\n",
      "ipykernel                          4.8.2     \r\n",
      "ipython                            6.5.0     \r\n",
      "ipython-genutils                   0.2.0     \r\n",
      "ipywidgets                         7.4.0     \r\n",
      "isort                              4.3.4     \r\n",
      "itsdangerous                       0.24      \r\n",
      "jdcal                              1.4       \r\n",
      "jedi                               0.12.1    \r\n",
      "jeepney                            0.3.1     \r\n",
      "Jinja2                             2.10      \r\n",
      "jsonschema                         2.6.0     \r\n",
      "jupyter                            1.0.0     \r\n",
      "jupyter-client                     5.2.3     \r\n",
      "jupyter-console                    5.2.0     \r\n",
      "jupyter-core                       4.4.0     \r\n",
      "jupyterlab                         0.34.1    \r\n",
      "jupyterlab-launcher                0.13.1    \r\n",
      "Keras                              2.2.4     \r\n",
      "Keras-Applications                 1.0.8     \r\n",
      "Keras-Preprocessing                1.1.0     \r\n",
      "keyring                            13.2.1    \r\n",
      "kiwisolver                         1.0.1     \r\n",
      "lazy-object-proxy                  1.3.1     \r\n",
      "List                               1.3.0     \r\n",
      "llvmlite                           0.24.0    \r\n",
      "locket                             0.2.0     \r\n",
      "logilab-common                     1.4.1     \r\n",
      "lxml                               4.2.4     \r\n",
      "Markdown                           3.1.1     \r\n",
      "MarkupSafe                         1.0       \r\n",
      "matplotlib                         2.2.3     \r\n",
      "mccabe                             0.6.1     \r\n",
      "mistune                            0.8.3     \r\n",
      "mkl-fft                            1.0.4     \r\n",
      "mkl-random                         1.0.1     \r\n",
      "mock                               2.0.0     \r\n",
      "more-itertools                     4.3.0     \r\n",
      "mpi4py                             2.0.0     \r\n",
      "mpmath                             1.0.0     \r\n",
      "msgpack                            0.5.6     \r\n",
      "multipledispatch                   0.6.0     \r\n",
      "navigator-updater                  0.2.1     \r\n",
      "nb-anacondacloud                   1.4.0     \r\n",
      "nb-conda                           2.2.1     \r\n",
      "nb-conda-kernels                   2.1.0     \r\n",
      "nbconvert                          5.3.1     \r\n",
      "nbformat                           4.4.0     \r\n",
      "nbpresent                          3.0.2     \r\n",
      "networkx                           2.1       \r\n",
      "nltk                               3.3       \r\n",
      "nose                               1.3.7     \r\n",
      "notebook                           5.6.0     \r\n",
      "numba                              0.39.0    \r\n",
      "numexpr                            2.6.7     \r\n",
      "numpy                              1.17.2    \r\n",
      "numpydoc                           0.8.0     \r\n",
      "odo                                0.5.1     \r\n",
      "olefile                            0.45.1    \r\n",
      "openpyxl                           2.5.5     \r\n",
      "opt-einsum                         3.1.0     \r\n",
      "packaging                          17.1      \r\n",
      "pandas                             0.23.4    \r\n",
      "pandocfilters                      1.4.2     \r\n",
      "parso                              0.3.1     \r\n",
      "partd                              0.3.8     \r\n",
      "path.py                            11.0.1    \r\n",
      "pathlib2                           2.3.2     \r\n",
      "patsy                              0.5.0     \r\n",
      "pbr                                4.2.0     \r\n",
      "pep8                               1.7.1     \r\n",
      "pexpect                            4.6.0     \r\n",
      "pickleshare                        0.7.4     \r\n",
      "Pillow                             5.2.0     \r\n",
      "pip                                19.2.3    \r\n",
      "pkginfo                            1.4.2     \r\n",
      "plotly                             3.6.1     \r\n",
      "pluggy                             0.7.1     \r\n",
      "ply                                3.11      \r\n",
      "prometheus-client                  0.3.1     \r\n",
      "prompt-toolkit                     1.0.15    \r\n",
      "protobuf                           3.9.2     \r\n",
      "psutil                             5.4.7     \r\n",
      "ptyprocess                         0.6.0     \r\n",
      "py                                 1.5.4     \r\n",
      "pyasn1                             0.4.4     \r\n",
      "pyasn1-modules                     0.2.2     \r\n",
      "pycairo                            1.15.4    \r\n",
      "pycodestyle                        2.3.1     \r\n",
      "pycosat                            0.6.3     \r\n",
      "pycparser                          2.18      \r\n",
      "pycrypto                           2.6.1     \r\n",
      "pycurl                             7.43.0.2  \r\n",
      "pydot                              1.2.4     \r\n",
      "pydot-ng                           2.0.0     \r\n",
      "pydotplus                          2.0.2     \r\n",
      "pyflakes                           1.6.0     \r\n",
      "Pygments                           2.2.0     \r\n",
      "PyHamcrest                         1.9.0     \r\n",
      "pylint                             2.1.1     \r\n",
      "pyodbc                             4.0.24    \r\n",
      "pyOpenSSL                          18.0.0    \r\n",
      "pyparsing                          2.2.0     \r\n",
      "PySocks                            1.6.8     \r\n",
      "pytest                             3.7.2     \r\n",
      "pytest-arraydiff                   0.2       \r\n",
      "pytest-astropy                     0.4.0     \r\n",
      "pytest-doctestplus                 0.1.3     \r\n",
      "pytest-openfiles                   0.3.0     \r\n",
      "pytest-remotedata                  0.3.0     \r\n",
      "python-dateutil                    2.7.3     \r\n",
      "pytz                               2018.5    \r\n",
      "PyWavelets                         0.5.2     \r\n",
      "PyYAML                             3.13      \r\n",
      "pyzmq                              17.1.2    \r\n",
      "QtAwesome                          0.4.4     \r\n",
      "qtconsole                          4.4.0     \r\n",
      "QtPy                               1.4.2     \r\n",
      "redis                              2.10.6    \r\n",
      "requests                           2.19.1    \r\n",
      "retrying                           1.3.3     \r\n",
      "rope                               0.11.0    \r\n",
      "ruamel-yaml                        0.15.46   \r\n",
      "scikit-image                       0.14.0    \r\n",
      "scikit-learn                       0.19.1    \r\n",
      "scipy                              1.1.0     \r\n",
      "seaborn                            0.9.0     \r\n",
      "SecretStorage                      3.0.1     \r\n",
      "Send2Trash                         1.5.0     \r\n",
      "service-identity                   17.0.0    \r\n",
      "setuptools                         41.2.0    \r\n",
      "simplegeneric                      0.8.1     \r\n",
      "simplejson                         3.16.0    \r\n",
      "singledispatch                     3.4.0.3   \r\n",
      "six                                1.11.0    \r\n",
      "snowballstemmer                    1.2.1     \r\n",
      "sockjs-tornado                     1.0.3     \r\n",
      "sortedcollections                  1.0.1     \r\n",
      "sortedcontainers                   2.0.4     \r\n",
      "Sphinx                             1.7.7     \r\n",
      "sphinx-rtd-theme                   0.4.1     \r\n",
      "sphinxcontrib-websupport           1.1.0     \r\n",
      "spyder                             3.3.1     \r\n",
      "spyder-kernels                     0.2.6     \r\n",
      "SQLAlchemy                         1.2.10    \r\n",
      "statsmodels                        0.9.0     \r\n",
      "sympy                              1.2       \r\n",
      "tables                             3.4.4     \r\n",
      "tabulate                           0.8.2     \r\n",
      "tblib                              1.3.2     \r\n",
      "tensorboard                        2.0.0     \r\n",
      "tensorflow-estimator               2.0.0     \r\n",
      "tensorflow-gpu                     1.9.0     \r\n",
      "tensorflow-tensorboard             1.5.1     \r\n",
      "termcolor                          1.1.0     \r\n",
      "terminado                          0.8.1     \r\n",
      "testpath                           0.3.1     \r\n",
      "toolz                              0.9.0     \r\n",
      "tornado                            5.1       \r\n",
      "traitlets                          4.3.2     \r\n",
      "Twisted                            18.7.0    \r\n",
      "typed-ast                          1.1.0     \r\n",
      "typing                             3.6.4     \r\n",
      "unicodecsv                         0.14.1    \r\n",
      "urllib3                            1.23      \r\n",
      "wcwidth                            0.1.7     \r\n",
      "webencodings                       0.5.1     \r\n",
      "Werkzeug                           0.14.1    \r\n",
      "wheel                              0.31.1    \r\n",
      "widgetsnbextension                 3.4.0     \r\n",
      "wrapt                              1.11.2    \r\n",
      "xlrd                               1.1.0     \r\n",
      "XlsxWriter                         1.0.7     \r\n",
      "xlwt                               1.3.0     \r\n",
      "zict                               0.1.3     \r\n",
      "zope.interface                     4.5.0     \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify package environment - if necessary\n",
    "If the above Keras and tensorflow packages were installed, you are good and can go to the next cell.\n",
    "\n",
    "If the above Keras and tensorflow packages were **not** installed, do the following:\n",
    "1. Open up a pitzer terminal window.\n",
    "2. Load the proper python by executing this: \n",
    "     * module load python/3.6-conda5.2\n",
    "3. Uninstall keras and tensorflow:\n",
    "     * pip uninstall tensorflow\n",
    "     * pip uninstall tensorflow-gpu\n",
    "     * pip uninstall keras\n",
    "4. Install the proper packages:\n",
    "     * pip install --user keras==2.2.4\n",
    "     * pip install --user tensorflow-gpu\n",
    "5. Come back to this noteboook and **restart** the kernel\n",
    "\n",
    "Now you can proceed with the rest of this notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "We will use our MNIST data sample yet again!   This time, we will use the version that comes along prepackaged with the keras package.\n",
    "\n",
    "Keras has a small number of datasets included as part of the package (see [here](https://keras.io/datasets/) for more details)   These include:\n",
    "1.  MNIST:  60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "2.  Reuters newswire topics classification:  11,228 newswires from Reuters, labeled over 46 topics, for text processing and classification. \n",
    "3.  CIFAR10 small image classification: Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.   There is a similar dataset (CIFAR100) with 100 labeled catagories.\n",
    "\n",
    "Below we load the MNIST dataset (both training and test).   We include an option for loading a \"short\" version to speed things up, but for real studies you should set short to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info (7000, 28, 28) (7000,)\n",
      "Test info (3000, 28, 28) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "short = True\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the feature data\n",
    "We need to make sure the feature data is:\n",
    "1. shaped appropriately. Eaach sample needs to be a 1D vector\n",
    "2. normalized.  Since we know our max and min is 255/0, we can just divide each pixel by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0],28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],28*28))\n",
    "test_images = test_images.astype('float32')/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the label data\n",
    "The labels run from 0-9, but we need to make them 1-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "Our model will be just like the one we build from scratch in assignment 7 prep:\n",
    "1. An input layer, 784 features wide.\n",
    "2. A hidden layer, 100 \"nodes\" wide, using the \"tanh\" activation function.\n",
    "3. An output layer, 10 modes wide, using the softmax activation function.\n",
    "\n",
    "Building this network with keras is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(100,activation='tanh',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "Compiling the model is necessary begfore you can train the model.  Compiling configures the learning process\n",
    "\n",
    "1.  A loss function. This is the objective that the model will try to minimize. There are a range of choices which can be examined [here](https://keras.io/losses/).   For classisifaction problems the typical choices are:\n",
    "    * categorical_crossentropy: used for multi-class classification (like MNIST)\n",
    "    * binary_crossentropy: used for binary classification (like any one vs all problem)\n",
    "2.  An optimizer. This controls how the minimum of the loss function is found.   SGD (stochastic gradient descent) is typical, as is Adam (see [here](https://arxiv.org/abs/1412.6980v8) for more details).\n",
    "3.  A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. \n",
    "\n",
    "Another thing we do below is to save the weights of the compiled network right after we first compile it.   These weight are initiailized to some random (and typically small) values.   This will be useful if we end up calling the network in an optimzation loop later.  For now, just make sure you do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# If we reload this right before fitting the model, the model will start from scratch\n",
    "network.save_weights('model_init.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "The \"fit\" method takes the following arguments:\n",
    "1.  The input features: in our case this is \"train_images\".\n",
    "2.  The output labels: input case this is the 1-hot \"train_labels_cat\"\n",
    "3.  The number of epochs to run.  Remember that an \"epoch\" is defined as an iteration in which the entire set of training samples has been passed through the network.   We use 50 below, but it is important to choose a number large enough that your performance (on the test set!) converges.   We will find that we might not want to use ALL of the epochs we give to the \"fit\" method - this is called \"early stopping\".   More on this below.\n",
    "4.  The batch size: this is the number of training samples that are passed through the network before the weights are updated.  Note the difference between this and the number of \"epochs\".  We will use 128 (typically). A good discussion of the issues surrounding batch size and epochs is found [here] (https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network).\n",
    "5.  An **optional** validation set.   This is a set of features and labels that are used to assess the performance of the model during the fit, at the end of each epoch.   Statistics on this (and the training set) are collected and returned when the fit is finished.\n",
    "\n",
    "The fit returns a **history** object, containing a .history dictionary with the following entries:\n",
    "*  history.history\\['loss'\\]: A list of the values of the loss function (evaluated on the training sample) at the end of each epoch, ordered by epoch.\n",
    "*  history.history\\['acc'\\]: A list of the values of the accuracy (evaluated on the training sample) at the end of each epoch, ordered by epoch.\n",
    "*  history.history\\['val_loss'\\]: A list of the values of the loss function (evaluated on the validation sample) at the end of each epoch, ordered by epoch.  Only returned if a validation sample is supplied.\n",
    "*  history.history\\['val_acc'\\]: A list of the values of the accuracy (evaluated on the validation sample) at the end of each epoch, ordered by epoch.  Only returned if a validation sample is supplied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs Validation vs Testing\n",
    "You may have noticed that we introduced a new concept called the \"validation\" sample.   This is sometimes confused with the \"testing\" sample, but they are different.\n",
    "\n",
    "To be clear:\n",
    "1.  **Training set**: A set of examples used for learning, that is to **fit** the values parameters (weights) of the classifier.\n",
    "\n",
    "2.  **Validation set**: A set of examples used to **tune**  the parameters (for example the number of nodes in the hidden layer) of a classifier.\n",
    "\n",
    "3.  **Test set**: A set of examples used only to assess the performance of an already fit classifier.\n",
    "\n",
    "If we do k-fold validation, we typically have *no* separate validation or testing set.   We split the training set up into k-folds, train on each fold and average the results.   Do this many times to choose our parameter setting (like the number of hidden nodes).   Once finished, we retrain our model using the **full** training sample.   Our expected performance is the average performance using the k-fold results (at the parameter setting we chose).\n",
    "\n",
    "For this MNIST data sample we will do something slightly different, since we have a large available training set:\n",
    "1.  We will use the MNIST **training** sample to supply data for our k-fold validation process - meaning this sample will be broken up into training and validation.\n",
    "2.  We will use the MNIST **testing** sample to test our fully trained sample, after k-fold validation.\n",
    "\n",
    "In the example fit below, we use the MNIST **training** sample, and split it into a single **temporary** training sample and a separate **validation** set in the \"fit\" function.  We will use the **test** sample from above separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images_temp,val_images,train_labels_cat_temp,val_labels_cat = train_test_split(train_images,train_labels_cat, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/100\n",
      "5600/5600 [==============================] - 0s 50us/step - loss: 1.0110 - acc: 0.7193 - val_loss: 0.5231 - val_acc: 0.8564\n",
      "Epoch 2/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.4445 - acc: 0.8829 - val_loss: 0.3768 - val_acc: 0.9021\n",
      "Epoch 3/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.3438 - acc: 0.9077 - val_loss: 0.3250 - val_acc: 0.9136\n",
      "Epoch 4/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.2934 - acc: 0.9221 - val_loss: 0.2940 - val_acc: 0.9186\n",
      "Epoch 5/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.2594 - acc: 0.9314 - val_loss: 0.2767 - val_acc: 0.9264\n",
      "Epoch 6/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.2346 - acc: 0.9384 - val_loss: 0.2667 - val_acc: 0.9250\n",
      "Epoch 7/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.2142 - acc: 0.9446 - val_loss: 0.2582 - val_acc: 0.9271\n",
      "Epoch 8/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.1935 - acc: 0.9480 - val_loss: 0.2471 - val_acc: 0.9286\n",
      "Epoch 9/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.1758 - acc: 0.9546 - val_loss: 0.2391 - val_acc: 0.9329\n",
      "Epoch 10/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.1626 - acc: 0.9570 - val_loss: 0.2346 - val_acc: 0.9357\n",
      "Epoch 11/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.1485 - acc: 0.9632 - val_loss: 0.2325 - val_acc: 0.9321\n",
      "Epoch 12/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1355 - acc: 0.9657 - val_loss: 0.2278 - val_acc: 0.9321\n",
      "Epoch 13/100\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.1241 - acc: 0.9709 - val_loss: 0.2261 - val_acc: 0.9329\n",
      "Epoch 14/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1138 - acc: 0.9734 - val_loss: 0.2256 - val_acc: 0.9300\n",
      "Epoch 15/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.1066 - acc: 0.9746 - val_loss: 0.2248 - val_acc: 0.9300\n",
      "Epoch 16/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0990 - acc: 0.9775 - val_loss: 0.2180 - val_acc: 0.9343\n",
      "Epoch 17/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0876 - acc: 0.9812 - val_loss: 0.2178 - val_acc: 0.9357\n",
      "Epoch 18/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0822 - acc: 0.9823 - val_loss: 0.2138 - val_acc: 0.9371\n",
      "Epoch 19/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0754 - acc: 0.9854 - val_loss: 0.2157 - val_acc: 0.9350\n",
      "Epoch 20/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0683 - acc: 0.9884 - val_loss: 0.2143 - val_acc: 0.9371\n",
      "Epoch 21/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0637 - acc: 0.9882 - val_loss: 0.2156 - val_acc: 0.9336\n",
      "Epoch 22/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0588 - acc: 0.9907 - val_loss: 0.2098 - val_acc: 0.9386\n",
      "Epoch 23/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0529 - acc: 0.9927 - val_loss: 0.2125 - val_acc: 0.9379\n",
      "Epoch 24/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0494 - acc: 0.9927 - val_loss: 0.2102 - val_acc: 0.9371\n",
      "Epoch 25/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0455 - acc: 0.9938 - val_loss: 0.2128 - val_acc: 0.9357\n",
      "Epoch 26/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0427 - acc: 0.9932 - val_loss: 0.2150 - val_acc: 0.9357\n",
      "Epoch 27/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0392 - acc: 0.9957 - val_loss: 0.2114 - val_acc: 0.9371\n",
      "Epoch 28/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0360 - acc: 0.9959 - val_loss: 0.2158 - val_acc: 0.9364\n",
      "Epoch 29/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0337 - acc: 0.9970 - val_loss: 0.2124 - val_acc: 0.9386\n",
      "Epoch 30/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0308 - acc: 0.9980 - val_loss: 0.2112 - val_acc: 0.9371\n",
      "Epoch 31/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0279 - acc: 0.9982 - val_loss: 0.2122 - val_acc: 0.9379\n",
      "Epoch 32/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0260 - acc: 0.9979 - val_loss: 0.2105 - val_acc: 0.9364\n",
      "Epoch 33/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9986 - val_loss: 0.2124 - val_acc: 0.9386\n",
      "Epoch 34/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9989 - val_loss: 0.2177 - val_acc: 0.9364\n",
      "Epoch 35/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9989 - val_loss: 0.2121 - val_acc: 0.9364\n",
      "Epoch 36/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9993 - val_loss: 0.2107 - val_acc: 0.9364\n",
      "Epoch 37/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9993 - val_loss: 0.2151 - val_acc: 0.9386\n",
      "Epoch 38/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9993 - val_loss: 0.2125 - val_acc: 0.9400\n",
      "Epoch 39/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9996 - val_loss: 0.2132 - val_acc: 0.9386\n",
      "Epoch 40/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9998 - val_loss: 0.2158 - val_acc: 0.9379\n",
      "Epoch 41/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9998 - val_loss: 0.2161 - val_acc: 0.9371\n",
      "Epoch 42/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0130 - acc: 0.9998 - val_loss: 0.2161 - val_acc: 0.9400\n",
      "Epoch 43/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.2161 - val_acc: 0.9414\n",
      "Epoch 44/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0114 - acc: 0.9998 - val_loss: 0.2185 - val_acc: 0.9386\n",
      "Epoch 45/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.2188 - val_acc: 0.9407\n",
      "Epoch 46/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.2207 - val_acc: 0.9400\n",
      "Epoch 47/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.2180 - val_acc: 0.9393\n",
      "Epoch 48/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2186 - val_acc: 0.9393\n",
      "Epoch 49/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9421\n",
      "Epoch 50/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.2206 - val_acc: 0.9421\n",
      "Epoch 51/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.9400\n",
      "Epoch 52/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2230 - val_acc: 0.9407\n",
      "Epoch 53/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2193 - val_acc: 0.9407\n",
      "Epoch 54/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 0.9400\n",
      "Epoch 55/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9407\n",
      "Epoch 56/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2247 - val_acc: 0.9393\n",
      "Epoch 57/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 0.9407\n",
      "Epoch 58/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2247 - val_acc: 0.9421\n",
      "Epoch 59/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2263 - val_acc: 0.9414\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.2256 - val_acc: 0.9407\n",
      "Epoch 61/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2275 - val_acc: 0.9414\n",
      "Epoch 62/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2265 - val_acc: 0.9421\n",
      "Epoch 63/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 0.9429\n",
      "Epoch 64/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9400\n",
      "Epoch 65/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9421\n",
      "Epoch 66/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2284 - val_acc: 0.9407\n",
      "Epoch 67/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2292 - val_acc: 0.9421\n",
      "Epoch 68/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.2298 - val_acc: 0.9407\n",
      "Epoch 69/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.9407\n",
      "Epoch 70/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.2302 - val_acc: 0.9429\n",
      "Epoch 71/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2312 - val_acc: 0.9414\n",
      "Epoch 72/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.2323 - val_acc: 0.9421\n",
      "Epoch 73/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.2315 - val_acc: 0.9429\n",
      "Epoch 74/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2345 - val_acc: 0.9429\n",
      "Epoch 75/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2333 - val_acc: 0.9407\n",
      "Epoch 76/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2340 - val_acc: 0.9429\n",
      "Epoch 77/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2334 - val_acc: 0.9429\n",
      "Epoch 78/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.2350 - val_acc: 0.9414\n",
      "Epoch 79/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2343 - val_acc: 0.9436\n",
      "Epoch 80/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2366 - val_acc: 0.9436\n",
      "Epoch 81/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2363 - val_acc: 0.9436\n",
      "Epoch 82/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2373 - val_acc: 0.9421\n",
      "Epoch 83/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.2364 - val_acc: 0.9421\n",
      "Epoch 84/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9414\n",
      "Epoch 85/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9443\n",
      "Epoch 86/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2378 - val_acc: 0.9414\n",
      "Epoch 87/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.2385 - val_acc: 0.9429\n",
      "Epoch 88/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.2398 - val_acc: 0.9407\n",
      "Epoch 89/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2401 - val_acc: 0.9436\n",
      "Epoch 90/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2414 - val_acc: 0.9429\n",
      "Epoch 91/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2412 - val_acc: 0.9450\n",
      "Epoch 92/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2411 - val_acc: 0.9450\n",
      "Epoch 93/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2428 - val_acc: 0.9421\n",
      "Epoch 94/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2432 - val_acc: 0.9414\n",
      "Epoch 95/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2431 - val_acc: 0.9443\n",
      "Epoch 96/100\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2446 - val_acc: 0.9429\n",
      "Epoch 97/100\n",
      "5600/5600 [==============================] - 0s 13us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2440 - val_acc: 0.9436\n",
      "Epoch 98/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2440 - val_acc: 0.9450\n",
      "Epoch 99/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2450 - val_acc: 0.9450\n",
      "Epoch 100/100\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2455 - val_acc: 0.9429\n"
     ]
    }
   ],
   "source": [
    "network.load_weights('model_init.h5')\n",
    "history = network.fit(train_images_temp,train_labels_cat_temp,epochs=100,batch_size=128,validation_data=(val_images,val_labels_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a model\n",
    "Once we have trained our model, we are ready to use it.  However, it often takes a **long time** to train a model, and once trained we may want to use it at a different time (and using a different python program).   Retraining the model right before we use it is not practical.  Instead, we will often save the model immediately upon training it, so we can simply **load** the already trained model into memory the next time we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('fully_trained_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine performance\n",
    "First let's look at the returned history object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 100\n",
      "Epoch\t Train Loss\t Train Acc\t Val Loss\t Val Acc\n",
      "1.01098 \t 0.71929 \t 0.52308 \t 0.85643\n",
      "0.44448 \t 0.88286 \t 0.37678 \t 0.90214\n",
      "0.34384 \t 0.90768 \t 0.325 \t 0.91357\n",
      "0.29343 \t 0.92214 \t 0.294 \t 0.91857\n",
      "0.25936 \t 0.93143 \t 0.27672 \t 0.92643\n",
      "0.23461 \t 0.93839 \t 0.26667 \t 0.925\n",
      "0.21422 \t 0.94464 \t 0.25825 \t 0.92714\n",
      "0.19348 \t 0.94804 \t 0.24709 \t 0.92857\n",
      "0.17585 \t 0.95464 \t 0.23915 \t 0.93286\n",
      "0.16259 \t 0.95696 \t 0.2346 \t 0.93571\n",
      "0.14847 \t 0.96321 \t 0.23247 \t 0.93214\n",
      "0.13547 \t 0.96571 \t 0.2278 \t 0.93214\n",
      "0.12413 \t 0.97089 \t 0.22609 \t 0.93286\n",
      "0.11383 \t 0.97339 \t 0.22561 \t 0.93\n",
      "0.10658 \t 0.97464 \t 0.22476 \t 0.93\n",
      "0.099 \t 0.9775 \t 0.21804 \t 0.93429\n",
      "0.0876 \t 0.98125 \t 0.21778 \t 0.93571\n",
      "0.08224 \t 0.98232 \t 0.21376 \t 0.93714\n",
      "0.07544 \t 0.98536 \t 0.2157 \t 0.935\n",
      "0.06835 \t 0.98839 \t 0.21434 \t 0.93714\n",
      "0.06367 \t 0.98821 \t 0.21564 \t 0.93357\n",
      "0.05879 \t 0.99071 \t 0.20984 \t 0.93857\n",
      "0.05288 \t 0.99268 \t 0.21247 \t 0.93786\n",
      "0.0494 \t 0.99268 \t 0.21017 \t 0.93714\n",
      "0.04548 \t 0.99375 \t 0.21281 \t 0.93571\n",
      "0.04274 \t 0.99321 \t 0.21504 \t 0.93571\n",
      "0.03915 \t 0.99571 \t 0.21144 \t 0.93714\n",
      "0.03602 \t 0.99589 \t 0.21583 \t 0.93643\n",
      "0.03374 \t 0.99696 \t 0.21237 \t 0.93857\n",
      "0.03084 \t 0.99804 \t 0.21118 \t 0.93714\n",
      "0.02789 \t 0.99821 \t 0.21222 \t 0.93786\n",
      "0.02603 \t 0.99786 \t 0.21045 \t 0.93643\n",
      "0.0242 \t 0.99857 \t 0.21237 \t 0.93857\n",
      "0.0224 \t 0.99893 \t 0.21773 \t 0.93643\n",
      "0.02116 \t 0.99893 \t 0.21215 \t 0.93643\n",
      "0.01944 \t 0.99929 \t 0.21068 \t 0.93643\n",
      "0.01814 \t 0.99929 \t 0.21507 \t 0.93857\n",
      "0.01697 \t 0.99929 \t 0.21254 \t 0.94\n",
      "0.01589 \t 0.99964 \t 0.21325 \t 0.93857\n",
      "0.01491 \t 0.99982 \t 0.21583 \t 0.93786\n",
      "0.01396 \t 0.99982 \t 0.21607 \t 0.93714\n",
      "0.01299 \t 0.99982 \t 0.21609 \t 0.94\n",
      "0.01211 \t 1.0 \t 0.21606 \t 0.94143\n",
      "0.0114 \t 0.99982 \t 0.21849 \t 0.93857\n",
      "0.01103 \t 1.0 \t 0.21878 \t 0.94071\n",
      "0.01022 \t 1.0 \t 0.22071 \t 0.94\n",
      "0.00959 \t 1.0 \t 0.218 \t 0.93929\n",
      "0.00908 \t 1.0 \t 0.21859 \t 0.93929\n",
      "0.00859 \t 1.0 \t 0.22088 \t 0.94214\n",
      "0.00809 \t 1.0 \t 0.22056 \t 0.94214\n",
      "0.00771 \t 1.0 \t 0.22157 \t 0.94\n",
      "0.00741 \t 1.0 \t 0.22304 \t 0.94071\n",
      "0.00702 \t 1.0 \t 0.21932 \t 0.94071\n",
      "0.00665 \t 1.0 \t 0.22218 \t 0.94\n",
      "0.00625 \t 1.0 \t 0.22203 \t 0.94071\n",
      "0.00602 \t 1.0 \t 0.22473 \t 0.93929\n",
      "0.00576 \t 1.0 \t 0.22321 \t 0.94071\n",
      "0.00548 \t 1.0 \t 0.22473 \t 0.94214\n",
      "0.00523 \t 1.0 \t 0.22631 \t 0.94143\n",
      "0.00497 \t 1.0 \t 0.22556 \t 0.94071\n",
      "0.00476 \t 1.0 \t 0.22754 \t 0.94143\n",
      "0.00459 \t 1.0 \t 0.2265 \t 0.94214\n",
      "0.00433 \t 1.0 \t 0.22679 \t 0.94286\n",
      "0.00418 \t 1.0 \t 0.22741 \t 0.94\n",
      "0.00395 \t 1.0 \t 0.22736 \t 0.94214\n",
      "0.00385 \t 1.0 \t 0.2284 \t 0.94071\n",
      "0.00366 \t 1.0 \t 0.22916 \t 0.94214\n",
      "0.00354 \t 1.0 \t 0.22977 \t 0.94071\n",
      "0.00339 \t 1.0 \t 0.23068 \t 0.94071\n",
      "0.00323 \t 1.0 \t 0.23022 \t 0.94286\n",
      "0.00313 \t 1.0 \t 0.23118 \t 0.94143\n",
      "0.00302 \t 1.0 \t 0.23233 \t 0.94214\n",
      "0.00288 \t 1.0 \t 0.23148 \t 0.94286\n",
      "0.00276 \t 1.0 \t 0.23454 \t 0.94286\n",
      "0.00269 \t 1.0 \t 0.23335 \t 0.94071\n",
      "0.00257 \t 1.0 \t 0.23402 \t 0.94286\n",
      "0.00249 \t 1.0 \t 0.23337 \t 0.94286\n",
      "0.0024 \t 1.0 \t 0.23503 \t 0.94143\n",
      "0.00231 \t 1.0 \t 0.23426 \t 0.94357\n",
      "0.00223 \t 1.0 \t 0.23665 \t 0.94357\n",
      "0.00215 \t 1.0 \t 0.23628 \t 0.94357\n",
      "0.00207 \t 1.0 \t 0.23735 \t 0.94214\n",
      "0.002 \t 1.0 \t 0.23638 \t 0.94214\n",
      "0.00193 \t 1.0 \t 0.23825 \t 0.94143\n",
      "0.00187 \t 1.0 \t 0.23817 \t 0.94429\n",
      "0.0018 \t 1.0 \t 0.23782 \t 0.94143\n",
      "0.00175 \t 1.0 \t 0.23855 \t 0.94286\n",
      "0.00169 \t 1.0 \t 0.23977 \t 0.94071\n",
      "0.00163 \t 1.0 \t 0.2401 \t 0.94357\n",
      "0.00157 \t 1.0 \t 0.24144 \t 0.94286\n",
      "0.00152 \t 1.0 \t 0.24119 \t 0.945\n",
      "0.00147 \t 1.0 \t 0.2411 \t 0.945\n",
      "0.00144 \t 1.0 \t 0.24283 \t 0.94214\n",
      "0.00138 \t 1.0 \t 0.24315 \t 0.94143\n",
      "0.00134 \t 1.0 \t 0.24313 \t 0.94429\n",
      "0.0013 \t 1.0 \t 0.24459 \t 0.94286\n",
      "0.00126 \t 1.0 \t 0.24398 \t 0.94357\n",
      "0.00122 \t 1.0 \t 0.24403 \t 0.945\n",
      "0.00118 \t 1.0 \t 0.24499 \t 0.945\n",
      "0.00115 \t 1.0 \t 0.24549 \t 0.94286\n"
     ]
    }
   ],
   "source": [
    "training_vals_acc = history.history['acc']\n",
    "training_vals_loss = history.history['loss']\n",
    "valid_vals_acc = history.history['val_acc']\n",
    "valid_vals_loss = history.history['val_loss']\n",
    "iterations = len(training_vals_acc)\n",
    "print(\"Number of iterations:\",iterations)\n",
    "print(\"Epoch\\t Train Loss\\t Train Acc\\t Val Loss\\t Val Acc\")\n",
    "for tl,ta,vl,va in zip(training_vals_loss,training_vals_acc,valid_vals_loss,valid_vals_acc):\n",
    "    print(round(tl,5),'\\t',round(ta,5),'\\t',round(vl,5),'\\t',round(va,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Performance\n",
    "Here we look at the train and validation performance versus epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_plotly_in_cell():\n",
    "  import IPython\n",
    "  from plotly.offline import init_notebook_mode\n",
    "#\n",
    "# OLD (google colab)\n",
    "#  display(IPython.core.display.HTML('''\n",
    "#        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "#  '''))\n",
    "#  init_notebook_mode(connected=False)\n",
    "#\n",
    "# New (OSC) [thanks to Stephen Gant for this!]\n",
    "  init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "markers",
         "name": "Train data",
         "type": "scatter",
         "uid": "26994350-4212-4743-8186-9660510bed21",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          1.0109805979047504,
          0.44448425088609966,
          0.34384101169449943,
          0.29342610137803216,
          0.2593629036630903,
          0.23460930994578771,
          0.21421883148806437,
          0.19347767685140882,
          0.17584679092679706,
          0.16258639284542628,
          0.14846839002200535,
          0.13546797931194304,
          0.12413254840033396,
          0.11382932301078524,
          0.10657895880086081,
          0.09900458761623927,
          0.08760458128792899,
          0.08223567579473769,
          0.07543616371495383,
          0.06834589145013265,
          0.06367236690861838,
          0.05879220713462149,
          0.05288234165736607,
          0.04940290212631226,
          0.04547608897089958,
          0.042742699001516615,
          0.03915195586425917,
          0.036020605734416414,
          0.0337379240989685,
          0.03084167242050171,
          0.027894226823534284,
          0.026033784012709345,
          0.024199355925832475,
          0.022395292978201593,
          0.021158455110022,
          0.019436199249965803,
          0.01813595777111394,
          0.01697003746671336,
          0.0158925046186362,
          0.014906817690602371,
          0.013961411389921392,
          0.012988045683928898,
          0.01211462642465319,
          0.011401364148727486,
          0.011025825430239949,
          0.010219768062233925,
          0.009591793811747006,
          0.0090784372442535,
          0.00858656440462385,
          0.008089853237782206,
          0.007705831421273095,
          0.0074087664884115965,
          0.0070218706556728905,
          0.006648330374487809,
          0.006253249392445598,
          0.006019238691244807,
          0.005763050943080868,
          0.005477862008182066,
          0.005228746434939759,
          0.004971994403749704,
          0.004757134959633862,
          0.004585933634745223,
          0.004333115704357624,
          0.004176061508644904,
          0.003953087138278143,
          0.0038453735703868524,
          0.003655243037002427,
          0.003540595845718469,
          0.0033943839743733406,
          0.003227673288700836,
          0.0031283373079661813,
          0.0030230540449598005,
          0.0028778326537992273,
          0.0027593339221285923,
          0.002688261391595006,
          0.002566811983872737,
          0.002489071751811675,
          0.0023953650745430163,
          0.002311523564026824,
          0.002234994139123176,
          0.0021516323887876103,
          0.0020710639382845587,
          0.001995049261354974,
          0.0019313824163483722,
          0.0018664229667878576,
          0.0018044673611543008,
          0.0017460528754496148,
          0.0016890079687748637,
          0.0016330049433080213,
          0.0015730088536760637,
          0.0015230585880843656,
          0.001471968153159001,
          0.0014386939756306154,
          0.0013825549577761974,
          0.0013409854232200554,
          0.0012951052378463957,
          0.0012551108155665653,
          0.001219322751276195,
          0.0011766446328588893,
          0.001151193840695279
         ]
        },
        {
         "mode": "markers",
         "name": "Test data",
         "type": "scatter",
         "uid": "3f366747-42e2-4e5e-8ab5-9d77aecf2c70",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.5230818319320679,
          0.3767778740610395,
          0.32500312490122657,
          0.2939981348173959,
          0.2767196535212653,
          0.2666689485311508,
          0.2582454899379185,
          0.24709205278328486,
          0.2391490238904953,
          0.23460151536124094,
          0.23247202387877872,
          0.2277992773907525,
          0.226088171516146,
          0.2256064934389932,
          0.22476021596363613,
          0.21804329863616398,
          0.2177834095273699,
          0.2137588869673865,
          0.21570246739046914,
          0.21434131605284554,
          0.21563920055116925,
          0.20984159324850354,
          0.21247340151241847,
          0.21017241656780242,
          0.21280751364571707,
          0.21503599422318595,
          0.21144083602087838,
          0.21583283688340868,
          0.21237273735659462,
          0.21117672886167255,
          0.2122161226613181,
          0.2104535333599363,
          0.2123728403023311,
          0.2177303364447185,
          0.2121461937257222,
          0.2106844792195729,
          0.21506573225770678,
          0.2125365675347192,
          0.21324876930032458,
          0.2158335349389485,
          0.21607122225420816,
          0.21608576442514146,
          0.21605936101504736,
          0.2184881806373596,
          0.21877849093505314,
          0.22071473853928703,
          0.21799577031816755,
          0.2185923045022147,
          0.22088128668921334,
          0.22056335066046034,
          0.2215695322411401,
          0.2230411740711757,
          0.21932122026171003,
          0.22218450844287874,
          0.22202761488301415,
          0.2247287390061787,
          0.2232084206172398,
          0.22472651847771236,
          0.22630699285439082,
          0.22555902225630625,
          0.22754407056740353,
          0.22649532181876048,
          0.226785352230072,
          0.22741423726081847,
          0.2273565411567688,
          0.2284042991910662,
          0.2291603082418442,
          0.22976719651903424,
          0.2306759866646358,
          0.23022495354924882,
          0.2311825397184917,
          0.23232690708977835,
          0.23147759548255376,
          0.23454425777707782,
          0.23334614285400937,
          0.23402328635965075,
          0.233366020492145,
          0.2350252046755382,
          0.2342601501941681,
          0.23664547264575958,
          0.23628122661794934,
          0.2373451451744352,
          0.23638158219201225,
          0.23824502170085907,
          0.23816541748387474,
          0.23782382488250733,
          0.2385481070620673,
          0.23977360946791512,
          0.24009863214833396,
          0.2414375125510352,
          0.24119032612868718,
          0.24109817317553928,
          0.24282515781266348,
          0.2431535381930215,
          0.2431269405569349,
          0.24459118562085289,
          0.2439799028635025,
          0.2440276609999793,
          0.24499497064522335,
          0.24548900646822794
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"0563b61b-a134-4bbd-9858-45427d0e656a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0563b61b-a134-4bbd-9858-45427d0e656a\", [{\"mode\": \"markers\", \"name\": \"Train data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [1.0109805979047504, 0.44448425088609966, 0.34384101169449943, 0.29342610137803216, 0.2593629036630903, 0.23460930994578771, 0.21421883148806437, 0.19347767685140882, 0.17584679092679706, 0.16258639284542628, 0.14846839002200535, 0.13546797931194304, 0.12413254840033396, 0.11382932301078524, 0.10657895880086081, 0.09900458761623927, 0.08760458128792899, 0.08223567579473769, 0.07543616371495383, 0.06834589145013265, 0.06367236690861838, 0.05879220713462149, 0.05288234165736607, 0.04940290212631226, 0.04547608897089958, 0.042742699001516615, 0.03915195586425917, 0.036020605734416414, 0.0337379240989685, 0.03084167242050171, 0.027894226823534284, 0.026033784012709345, 0.024199355925832475, 0.022395292978201593, 0.021158455110022, 0.019436199249965803, 0.01813595777111394, 0.01697003746671336, 0.0158925046186362, 0.014906817690602371, 0.013961411389921392, 0.012988045683928898, 0.01211462642465319, 0.011401364148727486, 0.011025825430239949, 0.010219768062233925, 0.009591793811747006, 0.0090784372442535, 0.00858656440462385, 0.008089853237782206, 0.007705831421273095, 0.0074087664884115965, 0.0070218706556728905, 0.006648330374487809, 0.006253249392445598, 0.006019238691244807, 0.005763050943080868, 0.005477862008182066, 0.005228746434939759, 0.004971994403749704, 0.004757134959633862, 0.004585933634745223, 0.004333115704357624, 0.004176061508644904, 0.003953087138278143, 0.0038453735703868524, 0.003655243037002427, 0.003540595845718469, 0.0033943839743733406, 0.003227673288700836, 0.0031283373079661813, 0.0030230540449598005, 0.0028778326537992273, 0.0027593339221285923, 0.002688261391595006, 0.002566811983872737, 0.002489071751811675, 0.0023953650745430163, 0.002311523564026824, 0.002234994139123176, 0.0021516323887876103, 0.0020710639382845587, 0.001995049261354974, 0.0019313824163483722, 0.0018664229667878576, 0.0018044673611543008, 0.0017460528754496148, 0.0016890079687748637, 0.0016330049433080213, 0.0015730088536760637, 0.0015230585880843656, 0.001471968153159001, 0.0014386939756306154, 0.0013825549577761974, 0.0013409854232200554, 0.0012951052378463957, 0.0012551108155665653, 0.001219322751276195, 0.0011766446328588893, 0.001151193840695279], \"type\": \"scatter\", \"uid\": \"f9752caa-4ef4-49bc-a874-9f9059a6dfe6\"}, {\"mode\": \"markers\", \"name\": \"Test data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.5230818319320679, 0.3767778740610395, 0.32500312490122657, 0.2939981348173959, 0.2767196535212653, 0.2666689485311508, 0.2582454899379185, 0.24709205278328486, 0.2391490238904953, 0.23460151536124094, 0.23247202387877872, 0.2277992773907525, 0.226088171516146, 0.2256064934389932, 0.22476021596363613, 0.21804329863616398, 0.2177834095273699, 0.2137588869673865, 0.21570246739046914, 0.21434131605284554, 0.21563920055116925, 0.20984159324850354, 0.21247340151241847, 0.21017241656780242, 0.21280751364571707, 0.21503599422318595, 0.21144083602087838, 0.21583283688340868, 0.21237273735659462, 0.21117672886167255, 0.2122161226613181, 0.2104535333599363, 0.2123728403023311, 0.2177303364447185, 0.2121461937257222, 0.2106844792195729, 0.21506573225770678, 0.2125365675347192, 0.21324876930032458, 0.2158335349389485, 0.21607122225420816, 0.21608576442514146, 0.21605936101504736, 0.2184881806373596, 0.21877849093505314, 0.22071473853928703, 0.21799577031816755, 0.2185923045022147, 0.22088128668921334, 0.22056335066046034, 0.2215695322411401, 0.2230411740711757, 0.21932122026171003, 0.22218450844287874, 0.22202761488301415, 0.2247287390061787, 0.2232084206172398, 0.22472651847771236, 0.22630699285439082, 0.22555902225630625, 0.22754407056740353, 0.22649532181876048, 0.226785352230072, 0.22741423726081847, 0.2273565411567688, 0.2284042991910662, 0.2291603082418442, 0.22976719651903424, 0.2306759866646358, 0.23022495354924882, 0.2311825397184917, 0.23232690708977835, 0.23147759548255376, 0.23454425777707782, 0.23334614285400937, 0.23402328635965075, 0.233366020492145, 0.2350252046755382, 0.2342601501941681, 0.23664547264575958, 0.23628122661794934, 0.2373451451744352, 0.23638158219201225, 0.23824502170085907, 0.23816541748387474, 0.23782382488250733, 0.2385481070620673, 0.23977360946791512, 0.24009863214833396, 0.2414375125510352, 0.24119032612868718, 0.24109817317553928, 0.24282515781266348, 0.2431535381930215, 0.2431269405569349, 0.24459118562085289, 0.2439799028635025, 0.2440276609999793, 0.24499497064522335, 0.24548900646822794], \"type\": \"scatter\", \"uid\": \"6297fc45-8e7a-4ecb-a5e9-083a945c8dee\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"0563b61b-a134-4bbd-9858-45427d0e656a\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0563b61b-a134-4bbd-9858-45427d0e656a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0563b61b-a134-4bbd-9858-45427d0e656a\", [{\"mode\": \"markers\", \"name\": \"Train data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [1.0109805979047504, 0.44448425088609966, 0.34384101169449943, 0.29342610137803216, 0.2593629036630903, 0.23460930994578771, 0.21421883148806437, 0.19347767685140882, 0.17584679092679706, 0.16258639284542628, 0.14846839002200535, 0.13546797931194304, 0.12413254840033396, 0.11382932301078524, 0.10657895880086081, 0.09900458761623927, 0.08760458128792899, 0.08223567579473769, 0.07543616371495383, 0.06834589145013265, 0.06367236690861838, 0.05879220713462149, 0.05288234165736607, 0.04940290212631226, 0.04547608897089958, 0.042742699001516615, 0.03915195586425917, 0.036020605734416414, 0.0337379240989685, 0.03084167242050171, 0.027894226823534284, 0.026033784012709345, 0.024199355925832475, 0.022395292978201593, 0.021158455110022, 0.019436199249965803, 0.01813595777111394, 0.01697003746671336, 0.0158925046186362, 0.014906817690602371, 0.013961411389921392, 0.012988045683928898, 0.01211462642465319, 0.011401364148727486, 0.011025825430239949, 0.010219768062233925, 0.009591793811747006, 0.0090784372442535, 0.00858656440462385, 0.008089853237782206, 0.007705831421273095, 0.0074087664884115965, 0.0070218706556728905, 0.006648330374487809, 0.006253249392445598, 0.006019238691244807, 0.005763050943080868, 0.005477862008182066, 0.005228746434939759, 0.004971994403749704, 0.004757134959633862, 0.004585933634745223, 0.004333115704357624, 0.004176061508644904, 0.003953087138278143, 0.0038453735703868524, 0.003655243037002427, 0.003540595845718469, 0.0033943839743733406, 0.003227673288700836, 0.0031283373079661813, 0.0030230540449598005, 0.0028778326537992273, 0.0027593339221285923, 0.002688261391595006, 0.002566811983872737, 0.002489071751811675, 0.0023953650745430163, 0.002311523564026824, 0.002234994139123176, 0.0021516323887876103, 0.0020710639382845587, 0.001995049261354974, 0.0019313824163483722, 0.0018664229667878576, 0.0018044673611543008, 0.0017460528754496148, 0.0016890079687748637, 0.0016330049433080213, 0.0015730088536760637, 0.0015230585880843656, 0.001471968153159001, 0.0014386939756306154, 0.0013825549577761974, 0.0013409854232200554, 0.0012951052378463957, 0.0012551108155665653, 0.001219322751276195, 0.0011766446328588893, 0.001151193840695279], \"type\": \"scatter\", \"uid\": \"f9752caa-4ef4-49bc-a874-9f9059a6dfe6\"}, {\"mode\": \"markers\", \"name\": \"Test data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.5230818319320679, 0.3767778740610395, 0.32500312490122657, 0.2939981348173959, 0.2767196535212653, 0.2666689485311508, 0.2582454899379185, 0.24709205278328486, 0.2391490238904953, 0.23460151536124094, 0.23247202387877872, 0.2277992773907525, 0.226088171516146, 0.2256064934389932, 0.22476021596363613, 0.21804329863616398, 0.2177834095273699, 0.2137588869673865, 0.21570246739046914, 0.21434131605284554, 0.21563920055116925, 0.20984159324850354, 0.21247340151241847, 0.21017241656780242, 0.21280751364571707, 0.21503599422318595, 0.21144083602087838, 0.21583283688340868, 0.21237273735659462, 0.21117672886167255, 0.2122161226613181, 0.2104535333599363, 0.2123728403023311, 0.2177303364447185, 0.2121461937257222, 0.2106844792195729, 0.21506573225770678, 0.2125365675347192, 0.21324876930032458, 0.2158335349389485, 0.21607122225420816, 0.21608576442514146, 0.21605936101504736, 0.2184881806373596, 0.21877849093505314, 0.22071473853928703, 0.21799577031816755, 0.2185923045022147, 0.22088128668921334, 0.22056335066046034, 0.2215695322411401, 0.2230411740711757, 0.21932122026171003, 0.22218450844287874, 0.22202761488301415, 0.2247287390061787, 0.2232084206172398, 0.22472651847771236, 0.22630699285439082, 0.22555902225630625, 0.22754407056740353, 0.22649532181876048, 0.226785352230072, 0.22741423726081847, 0.2273565411567688, 0.2284042991910662, 0.2291603082418442, 0.22976719651903424, 0.2306759866646358, 0.23022495354924882, 0.2311825397184917, 0.23232690708977835, 0.23147759548255376, 0.23454425777707782, 0.23334614285400937, 0.23402328635965075, 0.233366020492145, 0.2350252046755382, 0.2342601501941681, 0.23664547264575958, 0.23628122661794934, 0.2373451451744352, 0.23638158219201225, 0.23824502170085907, 0.23816541748387474, 0.23782382488250733, 0.2385481070620673, 0.23977360946791512, 0.24009863214833396, 0.2414375125510352, 0.24119032612868718, 0.24109817317553928, 0.24282515781266348, 0.2431535381930215, 0.2431269405569349, 0.24459118562085289, 0.2439799028635025, 0.2440276609999793, 0.24499497064522335, 0.24548900646822794], \"type\": \"scatter\", \"uid\": \"6297fc45-8e7a-4ecb-a5e9-083a945c8dee\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"0563b61b-a134-4bbd-9858-45427d0e656a\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "markers",
         "name": "Train data",
         "type": "scatter",
         "uid": "b0f59b99-5be6-463b-a055-fe7c39a8a55c",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.7192857142857143,
          0.8828571428571429,
          0.9076785710879735,
          0.9221428568022592,
          0.9314285710879735,
          0.9383928571428571,
          0.9446428574834551,
          0.9480357146263123,
          0.9546428574834551,
          0.9569642853736877,
          0.9632142853736877,
          0.9657142853736878,
          0.9708928571428571,
          0.9733928574834552,
          0.9746428574834551,
          0.9775,
          0.98125,
          0.9823214282308306,
          0.9853571428571428,
          0.9883928571428572,
          0.9882142853736877,
          0.9907142857142858,
          0.9926785714285714,
          0.9926785710879734,
          0.993750000340598,
          0.9932142860548837,
          0.9957142857142857,
          0.9958928571428571,
          0.9969642857142857,
          0.9980357142857142,
          0.9982142857142857,
          0.9978571425165449,
          0.9985714282308306,
          0.9989285714285714,
          0.9989285714285714,
          0.9992857142857143,
          0.9992857142857143,
          0.9992857142857143,
          0.9996428571428572,
          0.9998214285714285,
          0.9998214285714285,
          0.9998214285714285,
          1,
          0.9998214285714285,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        },
        {
         "mode": "markers",
         "name": "Test data",
         "type": "scatter",
         "uid": "8ae07001-c8ba-4ef0-9012-dc4bc4932db6",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8564285707473754,
          0.9021428568022591,
          0.9135714268684387,
          0.9185714275496346,
          0.9264285697255816,
          0.9249999989782061,
          0.9271428561210633,
          0.9285714268684387,
          0.932857141835349,
          0.9357142840112959,
          0.9321428561210632,
          0.9321428568022592,
          0.9328571425165448,
          0.929999999659402,
          0.929999999659402,
          0.9342857139451163,
          0.9357142860548837,
          0.9371428568022592,
          0.9349999996594021,
          0.9371428568022592,
          0.9335714282308306,
          0.9385714289120265,
          0.9378571425165448,
          0.9371428568022592,
          0.9357142853736877,
          0.9357142853736877,
          0.9371428574834552,
          0.9364285717691694,
          0.9385714289120265,
          0.9371428568022592,
          0.9378571431977408,
          0.9364285710879735,
          0.9385714282308306,
          0.9364285697255815,
          0.9364285717691694,
          0.9364285710879735,
          0.9385714275496346,
          0.9399999989782061,
          0.9385714275496346,
          0.9378571425165448,
          0.9371428568022592,
          0.9399999989782061,
          0.9414285704067775,
          0.9385714282308306,
          0.9407142846924919,
          0.9399999989782061,
          0.9392857146263123,
          0.9392857139451163,
          0.9421428561210632,
          0.9421428561210632,
          0.9399999989782061,
          0.9407142853736877,
          0.9407142846924919,
          0.9399999989782061,
          0.9407142846924919,
          0.9392857139451163,
          0.9407142846924919,
          0.9421428561210632,
          0.9414285704067775,
          0.9407142853736877,
          0.9414285704067775,
          0.9421428561210632,
          0.942857141835349,
          0.9399999996594021,
          0.9421428561210632,
          0.9407142846924919,
          0.9421428561210632,
          0.9407142846924919,
          0.9407142846924919,
          0.942857141835349,
          0.9414285704067775,
          0.9421428561210632,
          0.942857141835349,
          0.942857141154153,
          0.9407142853736877,
          0.942857141154153,
          0.942857141835349,
          0.9414285704067775,
          0.9435714275496346,
          0.9435714268684388,
          0.9435714275496346,
          0.9421428561210632,
          0.9421428561210632,
          0.9414285710879735,
          0.9442857125827244,
          0.9414285704067775,
          0.942857141835349,
          0.9407142846924919,
          0.9435714275496346,
          0.942857141154153,
          0.9449999982970102,
          0.9449999982970102,
          0.9421428554398673,
          0.9414285697255815,
          0.9442857125827244,
          0.942857141154153,
          0.9435714268684388,
          0.9449999982970102,
          0.9449999982970102,
          0.942857141154153
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"2973c381-06e8-45a5-945a-d9f6460665cb\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2973c381-06e8-45a5-945a-d9f6460665cb\", [{\"mode\": \"markers\", \"name\": \"Train data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.7192857142857143, 0.8828571428571429, 0.9076785710879735, 0.9221428568022592, 0.9314285710879735, 0.9383928571428571, 0.9446428574834551, 0.9480357146263123, 0.9546428574834551, 0.9569642853736877, 0.9632142853736877, 0.9657142853736878, 0.9708928571428571, 0.9733928574834552, 0.9746428574834551, 0.9775, 0.98125, 0.9823214282308306, 0.9853571428571428, 0.9883928571428572, 0.9882142853736877, 0.9907142857142858, 0.9926785714285714, 0.9926785710879734, 0.993750000340598, 0.9932142860548837, 0.9957142857142857, 0.9958928571428571, 0.9969642857142857, 0.9980357142857142, 0.9982142857142857, 0.9978571425165449, 0.9985714282308306, 0.9989285714285714, 0.9989285714285714, 0.9992857142857143, 0.9992857142857143, 0.9992857142857143, 0.9996428571428572, 0.9998214285714285, 0.9998214285714285, 0.9998214285714285, 1.0, 0.9998214285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"612674d8-1889-4d36-a532-4930a0412249\"}, {\"mode\": \"markers\", \"name\": \"Test data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.8564285707473754, 0.9021428568022591, 0.9135714268684387, 0.9185714275496346, 0.9264285697255816, 0.9249999989782061, 0.9271428561210633, 0.9285714268684387, 0.932857141835349, 0.9357142840112959, 0.9321428561210632, 0.9321428568022592, 0.9328571425165448, 0.929999999659402, 0.929999999659402, 0.9342857139451163, 0.9357142860548837, 0.9371428568022592, 0.9349999996594021, 0.9371428568022592, 0.9335714282308306, 0.9385714289120265, 0.9378571425165448, 0.9371428568022592, 0.9357142853736877, 0.9357142853736877, 0.9371428574834552, 0.9364285717691694, 0.9385714289120265, 0.9371428568022592, 0.9378571431977408, 0.9364285710879735, 0.9385714282308306, 0.9364285697255815, 0.9364285717691694, 0.9364285710879735, 0.9385714275496346, 0.9399999989782061, 0.9385714275496346, 0.9378571425165448, 0.9371428568022592, 0.9399999989782061, 0.9414285704067775, 0.9385714282308306, 0.9407142846924919, 0.9399999989782061, 0.9392857146263123, 0.9392857139451163, 0.9421428561210632, 0.9421428561210632, 0.9399999989782061, 0.9407142853736877, 0.9407142846924919, 0.9399999989782061, 0.9407142846924919, 0.9392857139451163, 0.9407142846924919, 0.9421428561210632, 0.9414285704067775, 0.9407142853736877, 0.9414285704067775, 0.9421428561210632, 0.942857141835349, 0.9399999996594021, 0.9421428561210632, 0.9407142846924919, 0.9421428561210632, 0.9407142846924919, 0.9407142846924919, 0.942857141835349, 0.9414285704067775, 0.9421428561210632, 0.942857141835349, 0.942857141154153, 0.9407142853736877, 0.942857141154153, 0.942857141835349, 0.9414285704067775, 0.9435714275496346, 0.9435714268684388, 0.9435714275496346, 0.9421428561210632, 0.9421428561210632, 0.9414285710879735, 0.9442857125827244, 0.9414285704067775, 0.942857141835349, 0.9407142846924919, 0.9435714275496346, 0.942857141154153, 0.9449999982970102, 0.9449999982970102, 0.9421428554398673, 0.9414285697255815, 0.9442857125827244, 0.942857141154153, 0.9435714268684388, 0.9449999982970102, 0.9449999982970102, 0.942857141154153], \"type\": \"scatter\", \"uid\": \"7955e190-f7a7-4c60-a2a2-505bc3ca583a\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"2973c381-06e8-45a5-945a-d9f6460665cb\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"2973c381-06e8-45a5-945a-d9f6460665cb\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2973c381-06e8-45a5-945a-d9f6460665cb\", [{\"mode\": \"markers\", \"name\": \"Train data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.7192857142857143, 0.8828571428571429, 0.9076785710879735, 0.9221428568022592, 0.9314285710879735, 0.9383928571428571, 0.9446428574834551, 0.9480357146263123, 0.9546428574834551, 0.9569642853736877, 0.9632142853736877, 0.9657142853736878, 0.9708928571428571, 0.9733928574834552, 0.9746428574834551, 0.9775, 0.98125, 0.9823214282308306, 0.9853571428571428, 0.9883928571428572, 0.9882142853736877, 0.9907142857142858, 0.9926785714285714, 0.9926785710879734, 0.993750000340598, 0.9932142860548837, 0.9957142857142857, 0.9958928571428571, 0.9969642857142857, 0.9980357142857142, 0.9982142857142857, 0.9978571425165449, 0.9985714282308306, 0.9989285714285714, 0.9989285714285714, 0.9992857142857143, 0.9992857142857143, 0.9992857142857143, 0.9996428571428572, 0.9998214285714285, 0.9998214285714285, 0.9998214285714285, 1.0, 0.9998214285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"612674d8-1889-4d36-a532-4930a0412249\"}, {\"mode\": \"markers\", \"name\": \"Test data\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], \"y\": [0.8564285707473754, 0.9021428568022591, 0.9135714268684387, 0.9185714275496346, 0.9264285697255816, 0.9249999989782061, 0.9271428561210633, 0.9285714268684387, 0.932857141835349, 0.9357142840112959, 0.9321428561210632, 0.9321428568022592, 0.9328571425165448, 0.929999999659402, 0.929999999659402, 0.9342857139451163, 0.9357142860548837, 0.9371428568022592, 0.9349999996594021, 0.9371428568022592, 0.9335714282308306, 0.9385714289120265, 0.9378571425165448, 0.9371428568022592, 0.9357142853736877, 0.9357142853736877, 0.9371428574834552, 0.9364285717691694, 0.9385714289120265, 0.9371428568022592, 0.9378571431977408, 0.9364285710879735, 0.9385714282308306, 0.9364285697255815, 0.9364285717691694, 0.9364285710879735, 0.9385714275496346, 0.9399999989782061, 0.9385714275496346, 0.9378571425165448, 0.9371428568022592, 0.9399999989782061, 0.9414285704067775, 0.9385714282308306, 0.9407142846924919, 0.9399999989782061, 0.9392857146263123, 0.9392857139451163, 0.9421428561210632, 0.9421428561210632, 0.9399999989782061, 0.9407142853736877, 0.9407142846924919, 0.9399999989782061, 0.9407142846924919, 0.9392857139451163, 0.9407142846924919, 0.9421428561210632, 0.9414285704067775, 0.9407142853736877, 0.9414285704067775, 0.9421428561210632, 0.942857141835349, 0.9399999996594021, 0.9421428561210632, 0.9407142846924919, 0.9421428561210632, 0.9407142846924919, 0.9407142846924919, 0.942857141835349, 0.9414285704067775, 0.9421428561210632, 0.942857141835349, 0.942857141154153, 0.9407142853736877, 0.942857141154153, 0.942857141835349, 0.9414285704067775, 0.9435714275496346, 0.9435714268684388, 0.9435714275496346, 0.9421428561210632, 0.9421428561210632, 0.9414285710879735, 0.9442857125827244, 0.9414285704067775, 0.942857141835349, 0.9407142846924919, 0.9435714275496346, 0.942857141154153, 0.9449999982970102, 0.9449999982970102, 0.9421428554398673, 0.9414285697255815, 0.9442857125827244, 0.942857141154153, 0.9435714268684388, 0.9449999982970102, 0.9449999982970102, 0.942857141154153], \"type\": \"scatter\", \"uid\": \"7955e190-f7a7-4c60-a2a2-505bc3ca583a\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"2973c381-06e8-45a5-945a-d9f6460665cb\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "enable_plotly_in_cell()\n",
    "#\n",
    "# Costs\n",
    "data_train = go.Scatter(\n",
    "    x=np.array(range(0,len(history.history['loss']))),\n",
    "    y=history.history['loss'],\n",
    "    mode='markers',\n",
    "    name=\"Train data\"\n",
    ")\n",
    "data_test = go.Scatter(\n",
    "    x=np.array(range(0,len(history.history['val_loss']))),\n",
    "    y=history.history['val_loss'],\n",
    "    mode='markers',\n",
    "    name=\"Test data\"\n",
    ")\n",
    "iplot(dict(data=[data_train,data_test]))\n",
    "\n",
    "#\n",
    "# Costs\n",
    "data_train = go.Scatter(\n",
    "    x=np.array(range(0,len(history.history['acc']))),\n",
    "    y=history.history['acc'],\n",
    "    mode='markers',\n",
    "    name=\"Train data\"\n",
    ")\n",
    "data_test = go.Scatter(\n",
    "    x=np.array(range(0,len(history.history['val_acc']))),\n",
    "    y=history.history['val_acc'],\n",
    "    mode='markers',\n",
    "    name=\"Test data\"\n",
    ")\n",
    "iplot(dict(data=[data_train,data_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained network\n",
    "Here we will load the pretrained network (deleting the version in memory to prove that this works!), and then apply this network to unseen data - our testing sample that we loaded above.\n",
    "\n",
    "To get the network performance, we have two options:\n",
    "1.  network.evaluate: This we use if we have labeled samples.   It returns the overall loss, as well as the calculated accuracy on that labeled dataset.\n",
    "2.  network.predict:  This can be used on labeled or unlabeled data.  It returns the output of the network (in our case the 10 probabilities for the 10 classes) for each sample.  If you do have labeled data, you can compare the predicted output to the known label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 24us/step\n",
      "Test sample loss:  0.36053816095987956 ; Test sample accuracy:  0.916666666507721\n",
      "Label\t Pred\t Prob\n",
      "7 \t 7 \t 1.0\n",
      "2 \t 2 \t 0.996\n",
      "1 \t 1 \t 1.0\n",
      "0 \t 0 \t 1.0\n",
      "4 \t 4 \t 1.0\n",
      "1 \t 1 \t 1.0\n",
      "4 \t 4 \t 1.0\n",
      "9 \t 9 \t 1.0\n",
      "5 \t 6 \t 0.84\n",
      "9 \t 9 \t 1.0\n",
      "0 \t 0 \t 1.0\n",
      "6 \t 6 \t 1.0\n",
      "9 \t 9 \t 1.0\n",
      "0 \t 0 \t 1.0\n",
      "1 \t 1 \t 1.0\n",
      "5 \t 5 \t 0.999\n",
      "9 \t 9 \t 1.0\n",
      "7 \t 7 \t 1.0\n",
      "3 \t 3 \t 0.782\n",
      "4 \t 4 \t 1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "#\n",
    "# Delete the current model if it exists\n",
    "try:\n",
    "    del network  # deletes the existing model\n",
    "except:\n",
    "    print(\"network already deleted\")\n",
    "    \n",
    "# returns a compiled model\n",
    "# identical to the previous one (note the new name!!)\n",
    "trained_network = load_model('fully_trained_model.h5')\n",
    "#\n",
    "# Get the overall performance for the test sample\n",
    "test_loss, test_acc = trained_network.evaluate(test_images,test_labels_cat)\n",
    "print(\"Test sample loss: \",test_loss, \"; Test sample accuracy: \",test_acc)\n",
    "#\n",
    "# Get the individual predictions for each sample in the test set\n",
    "predictions = trained_network.predict(test_images)\n",
    "#\n",
    "# Get the max probabilites for each rows\n",
    "probs = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "classes = np.argmax(predictions, axis = 1)\n",
    "#\n",
    "# Now loop over the first twenty samples and compare truth to prediction\n",
    "print(\"Label\\t Pred\\t Prob\")\n",
    "for label,cl,pr in zip(test_labels[:20],classes[:20],probs[:20]):\n",
    "    print(label,'\\t',cl,'\\t',round(pr,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Notice that in the loss plot above, the network performance was best somewhere in the range of epochs 10-20, yet we continued to train the network until epoch 50.   Keras makes it possible to do two things:\n",
    "1.  Stop the training once a condition has been met, using a module called \"EarlyStopping\".   This has two parameters:\n",
    "   * what is monitored for stopping: we will use 'val_loss' the loss in the validation set.\n",
    "   * \"patience\": this is how many epochs to wait after the condition has been met.  The idea being that there are fluctuations in the parameter you are monitoring, and you don't want to stop if you just had a small downward fluctuation.   So you wait a few epochs to see if the performance does not get better.\n",
    "2.  Save the best network prior to stopping, using a module called \"ModelCheckpoint\".   You tell this module what to monitor, and every time the condition is met, you write out a (and overwrite the previous) new file containing the full model info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/50\n",
      "5600/5600 [==============================] - 0s 45us/step - loss: 1.0361 - acc: 0.7041 - val_loss: 0.5216 - val_acc: 0.8664\n",
      "Epoch 2/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.4414 - acc: 0.8852 - val_loss: 0.3810 - val_acc: 0.9079\n",
      "Epoch 3/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.3454 - acc: 0.9080 - val_loss: 0.3332 - val_acc: 0.9186\n",
      "Epoch 4/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.2966 - acc: 0.9227 - val_loss: 0.3003 - val_acc: 0.9229\n",
      "Epoch 5/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.2614 - acc: 0.9293 - val_loss: 0.2844 - val_acc: 0.9243\n",
      "Epoch 6/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.2334 - acc: 0.9409 - val_loss: 0.2667 - val_acc: 0.9314\n",
      "Epoch 7/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.2129 - acc: 0.9436 - val_loss: 0.2616 - val_acc: 0.9307\n",
      "Epoch 8/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1927 - acc: 0.9489 - val_loss: 0.2507 - val_acc: 0.9329\n",
      "Epoch 9/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1762 - acc: 0.9561 - val_loss: 0.2481 - val_acc: 0.9321\n",
      "Epoch 10/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1599 - acc: 0.9570 - val_loss: 0.2468 - val_acc: 0.9314\n",
      "Epoch 11/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1472 - acc: 0.9630 - val_loss: 0.2379 - val_acc: 0.9379\n",
      "Epoch 12/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.1356 - acc: 0.9662 - val_loss: 0.2333 - val_acc: 0.9314\n",
      "Epoch 13/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.1225 - acc: 0.9700 - val_loss: 0.2317 - val_acc: 0.9357\n",
      "Epoch 14/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.1126 - acc: 0.9732 - val_loss: 0.2253 - val_acc: 0.9364\n",
      "Epoch 15/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.1041 - acc: 0.9741 - val_loss: 0.2249 - val_acc: 0.9350\n",
      "Epoch 16/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0951 - acc: 0.9780 - val_loss: 0.2236 - val_acc: 0.9379\n",
      "Epoch 17/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0865 - acc: 0.9816 - val_loss: 0.2210 - val_acc: 0.9357\n",
      "Epoch 18/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0791 - acc: 0.9838 - val_loss: 0.2197 - val_acc: 0.9371\n",
      "Epoch 19/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0729 - acc: 0.9857 - val_loss: 0.2215 - val_acc: 0.9386\n",
      "Epoch 20/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0679 - acc: 0.9866 - val_loss: 0.2163 - val_acc: 0.9414\n",
      "Epoch 21/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0616 - acc: 0.9902 - val_loss: 0.2192 - val_acc: 0.9379\n",
      "Epoch 22/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0569 - acc: 0.9912 - val_loss: 0.2167 - val_acc: 0.9379\n",
      "Epoch 23/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0515 - acc: 0.9923 - val_loss: 0.2196 - val_acc: 0.9393\n",
      "Epoch 24/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0483 - acc: 0.9932 - val_loss: 0.2144 - val_acc: 0.9407\n",
      "Epoch 25/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0437 - acc: 0.9943 - val_loss: 0.2141 - val_acc: 0.9407\n",
      "Epoch 26/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0404 - acc: 0.9959 - val_loss: 0.2145 - val_acc: 0.9407\n",
      "Epoch 27/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0377 - acc: 0.9955 - val_loss: 0.2157 - val_acc: 0.9407\n",
      "Epoch 28/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0340 - acc: 0.9970 - val_loss: 0.2142 - val_acc: 0.9407\n",
      "Epoch 29/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0316 - acc: 0.9975 - val_loss: 0.2125 - val_acc: 0.9407\n",
      "Epoch 30/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0294 - acc: 0.9979 - val_loss: 0.2184 - val_acc: 0.9393\n",
      "Epoch 31/50\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0268 - acc: 0.9986 - val_loss: 0.2181 - val_acc: 0.9386\n",
      "Epoch 32/50\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0249 - acc: 0.9984 - val_loss: 0.2169 - val_acc: 0.9414\n",
      "Epoch 33/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0234 - acc: 0.9988 - val_loss: 0.2196 - val_acc: 0.9407\n",
      "Epoch 34/50\n",
      "5600/5600 [==============================] - 0s 16us/step - loss: 0.0215 - acc: 0.9991 - val_loss: 0.2235 - val_acc: 0.9407\n",
      "Epoch 35/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9407\n",
      "Epoch 36/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9996 - val_loss: 0.2196 - val_acc: 0.9393\n",
      "Epoch 37/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9995 - val_loss: 0.2233 - val_acc: 0.9393\n",
      "Epoch 38/50\n",
      "5600/5600 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9996 - val_loss: 0.2214 - val_acc: 0.9393\n",
      "Epoch 39/50\n",
      "5600/5600 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9996 - val_loss: 0.2232 - val_acc: 0.9400\n",
      "Number of iterations: 39\n",
      "Epoch\t Train Loss\t Train Acc\t Val Loss\t Val Acc\n",
      "0 \t 1.03613 \t 0.70411 \t 0.52165 \t 0.86643\n",
      "1 \t 0.44139 \t 0.88518 \t 0.38103 \t 0.90786\n",
      "2 \t 0.34545 \t 0.90804 \t 0.33323 \t 0.91857\n",
      "3 \t 0.29662 \t 0.92268 \t 0.30028 \t 0.92286\n",
      "4 \t 0.26138 \t 0.92929 \t 0.28444 \t 0.92429\n",
      "5 \t 0.23341 \t 0.94089 \t 0.26669 \t 0.93143\n",
      "6 \t 0.2129 \t 0.94357 \t 0.26161 \t 0.93071\n",
      "7 \t 0.19268 \t 0.94893 \t 0.25075 \t 0.93286\n",
      "8 \t 0.17624 \t 0.95607 \t 0.24805 \t 0.93214\n",
      "9 \t 0.15988 \t 0.95696 \t 0.24675 \t 0.93143\n",
      "10 \t 0.14715 \t 0.96304 \t 0.23791 \t 0.93786\n",
      "11 \t 0.13558 \t 0.96625 \t 0.23325 \t 0.93143\n",
      "12 \t 0.1225 \t 0.97 \t 0.23166 \t 0.93571\n",
      "13 \t 0.11257 \t 0.97321 \t 0.22535 \t 0.93643\n",
      "14 \t 0.10407 \t 0.97411 \t 0.22493 \t 0.935\n",
      "15 \t 0.09506 \t 0.97804 \t 0.22359 \t 0.93786\n",
      "16 \t 0.08652 \t 0.98161 \t 0.22103 \t 0.93571\n",
      "17 \t 0.07907 \t 0.98375 \t 0.2197 \t 0.93714\n",
      "18 \t 0.07291 \t 0.98571 \t 0.22146 \t 0.93857\n",
      "19 \t 0.06788 \t 0.98661 \t 0.21632 \t 0.94143\n",
      "20 \t 0.06164 \t 0.99018 \t 0.21922 \t 0.93786\n",
      "21 \t 0.05691 \t 0.99125 \t 0.21668 \t 0.93786\n",
      "22 \t 0.05151 \t 0.99232 \t 0.2196 \t 0.93929\n",
      "23 \t 0.04831 \t 0.99321 \t 0.21436 \t 0.94071\n",
      "24 \t 0.04367 \t 0.99429 \t 0.21406 \t 0.94071\n",
      "25 \t 0.04044 \t 0.99589 \t 0.21446 \t 0.94071\n",
      "26 \t 0.03767 \t 0.99554 \t 0.21567 \t 0.94071\n",
      "27 \t 0.03401 \t 0.99696 \t 0.21417 \t 0.94071\n",
      "28 \t 0.03162 \t 0.9975 \t 0.2125 \t 0.94071\n",
      "29 \t 0.02939 \t 0.99786 \t 0.21835 \t 0.93929\n",
      "30 \t 0.02685 \t 0.99857 \t 0.21808 \t 0.93857\n",
      "31 \t 0.02486 \t 0.99839 \t 0.21688 \t 0.94143\n",
      "32 \t 0.02337 \t 0.99875 \t 0.21958 \t 0.94071\n",
      "33 \t 0.02154 \t 0.99911 \t 0.22352 \t 0.94071\n",
      "34 \t 0.01999 \t 0.99911 \t 0.22044 \t 0.94071\n",
      "35 \t 0.01847 \t 0.99964 \t 0.21965 \t 0.93929\n",
      "36 \t 0.01724 \t 0.99946 \t 0.22335 \t 0.93929\n",
      "37 \t 0.01611 \t 0.99964 \t 0.22139 \t 0.93929\n",
      "38 \t 0.01515 \t 0.99964 \t 0.22319 \t 0.94\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "hidden_nodes = 100\n",
    "activation = 'tanh'\n",
    "optimizer = 'adam'\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(hidden_nodes,activation=activation,input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))\n",
    "network.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# If we reload this right before fitting the model, the model will start from scratch\n",
    "network.save_weights('model_init.h5')\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "network.load_weights('model_init.h5')\n",
    "history = network.fit(train_images_temp,train_labels_cat_temp,\n",
    "                              epochs=50,\n",
    "                              batch_size=128,\n",
    "                              verbose=1, # set to 0 for no printout while running\n",
    "                              callbacks=callbacks, # Early stopping\n",
    "                              validation_data=(val_images,val_labels_cat))\n",
    "#\n",
    "# get performance info\n",
    "training_vals_acc = history.history['acc']\n",
    "training_vals_loss = history.history['loss']\n",
    "valid_vals_acc = history.history['val_acc']\n",
    "valid_vals_loss = history.history['val_loss']\n",
    "iterations = len(training_vals_acc)\n",
    "print(\"Number of iterations:\",iterations)\n",
    "print(\"Epoch\\t Train Loss\\t Train Acc\\t Val Loss\\t Val Acc\")\n",
    "i = 0\n",
    "for tl,ta,vl,va in zip(training_vals_loss,training_vals_acc,valid_vals_loss,valid_vals_acc):\n",
    "    print(i,'\\t',round(tl,5),'\\t',round(ta,5),'\\t',round(vl,5),'\\t',round(va,5))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996428571428572"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vals_acc[iterations-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7041071428571428,\n",
       " 0.8851785717691694,\n",
       " 0.9080357146263123,\n",
       " 0.9226785714285715,\n",
       " 0.9292857139451163,\n",
       " 0.9408928568022592,\n",
       " 0.9435714282308306,\n",
       " 0.9489285717691694,\n",
       " 0.9560714289120266,\n",
       " 0.9569642860548837,\n",
       " 0.9630357142857143,\n",
       " 0.9662499996594021,\n",
       " 0.969999999659402,\n",
       " 0.9732142853736877,\n",
       " 0.9741071428571428,\n",
       " 0.9780357142857142,\n",
       " 0.9816071425165449,\n",
       " 0.983750000340598,\n",
       " 0.9857142860548836,\n",
       " 0.9866071428571429,\n",
       " 0.9901785710879735,\n",
       " 0.99125,\n",
       " 0.9923214285714286,\n",
       " 0.9932142857142857,\n",
       " 0.9942857139451163,\n",
       " 0.9958928568022591,\n",
       " 0.9955357142857143,\n",
       " 0.9969642857142857,\n",
       " 0.9975,\n",
       " 0.9978571425165449,\n",
       " 0.9985714285714286,\n",
       " 0.9983928571428572,\n",
       " 0.99875,\n",
       " 0.9991071428571429,\n",
       " 0.9991071428571429,\n",
       " 0.9996428571428572,\n",
       " 0.9994642857142857,\n",
       " 0.9996428571428572,\n",
       " 0.9996428571428572]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vals_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTICE**: Training stopped at XX epochs (the exact number is somewhat random), not 50, and the minimum validation sample loss was at epoch XX-10 (training continued for patience=10 epochs after this minimum to make sure we did not hit yet another minimum).   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
